#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
Comprehensive Persian Data Analysis Script

Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ÛŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
This script provides comprehensive analysis of Persian text data
"""

import os
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter, defaultdict
import json
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ØªÙ†Ø¸ÛŒÙ… ÙÙˆÙ†Øª ÙØ§Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ matplotlib
plt.rcParams['font.family'] = ['Tahoma', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PersianDataAnalyzer:
    def __init__(self, data_directory="."):
        """
        Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        Persian Data Analyzer Class
        """
        self.data_directory = data_directory
        self.datasets = {}
        self.analysis_results = {}
        self.persian_stopwords = {
            'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ø²', 'Ø¯Ø±', 'Ø¨Ø§', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ùˆ', 'ÛŒØ§', 'ØªØ§', 
            'Ø¨Ø±Ø§ÛŒ', 'Ø±ÙˆÛŒ', 'Ø²ÛŒØ±', 'Ú©Ù†Ø§Ø±', 'Ù¾ÛŒØ´', 'Ù†Ø²Ø¯', 'Ù…ÛŒØ§Ù†', 'Ø¨ÛŒÙ†'
        }
        
    def load_data(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ"""
        print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        file_patterns = {
            'train': 'mc_train_data.txt',
            'dev': 'mc_dev_data.txt', 
            'test': 'mc_test_data.txt'
        }
        
        for dataset_name, filename in file_patterns.items():
            filepath = os.path.join(self.data_directory, filename)
            if os.path.exists(filepath):
                data = []
                with open(filepath, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split(' ', 1)
                            if len(parts) == 2:
                                label = int(parts[0])
                                text = parts[1]
                                data.append({'label': label, 'text': text})
                
                self.datasets[dataset_name] = pd.DataFrame(data)
                print(f"âœ… {dataset_name}: {len(data)} Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            else:
                print(f"âŒ ÙØ§ÛŒÙ„ {filename} ÛŒØ§ÙØª Ù†Ø´Ø¯")
    
    def preprocess_text(self, text):
        """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
        # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
        text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
        text = re.sub(r'\s+', ' ', text)
        # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§
        text = text.strip()
        return text
    
    def extract_words(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ø§Ø² Ù…ØªÙ†"""
        text = self.preprocess_text(text)
        words = text.split()
        # Ø­Ø°Ù stop words
        words = [word for word in words if word not in self.persian_stopwords]
        return words
    
    def basic_statistics(self):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡"""
        print("\nğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡...")
        
        stats = {}
        for dataset_name, df in self.datasets.items():
            dataset_stats = {}
            
            # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
            dataset_stats['total_samples'] = len(df)
            dataset_stats['label_distribution'] = df['label'].value_counts().to_dict()
            dataset_stats['label_percentage'] = (df['label'].value_counts(normalize=True) * 100).to_dict()
            
            # Ø¢Ù…Ø§Ø± Ù…ØªÙ†
            df['text_length'] = df['text'].str.len()
            df['word_count'] = df['text'].apply(lambda x: len(self.extract_words(x)))
            
            dataset_stats['text_length'] = {
                'mean': df['text_length'].mean(),
                'median': df['text_length'].median(),
                'std': df['text_length'].std(),
                'min': df['text_length'].min(),
                'max': df['text_length'].max()
            }
            
            dataset_stats['word_count'] = {
                'mean': df['word_count'].mean(),
                'median': df['word_count'].median(),
                'std': df['word_count'].std(),
                'min': df['word_count'].min(),
                'max': df['word_count'].max()
            }
            
            stats[dataset_name] = dataset_stats
        
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def analyze_vocabulary(self):
        """ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†"""
        print("\nğŸ“š Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†...")
        
        vocab_analysis = {}
        
        for dataset_name, df in self.datasets.items():
            all_words = []
            words_by_label = {0: [], 1: []}
            
            for _, row in df.iterrows():
                words = self.extract_words(row['text'])
                all_words.extend(words)
                words_by_label[row['label']].extend(words)
            
            vocab_stats = {}
            vocab_stats['total_words'] = len(all_words)
            vocab_stats['unique_words'] = len(set(all_words))
            vocab_stats['vocabulary_richness'] = len(set(all_words)) / len(all_words) if all_words else 0
            
            # Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±
            word_freq = Counter(all_words)
            vocab_stats['most_common_words'] = word_freq.most_common(20)
            
            # ØªØ­Ù„ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
            label_vocab = {}
            for label in [0, 1]:
                label_words = words_by_label[label]
                if label_words:
                    label_freq = Counter(label_words)
                    label_vocab[label] = {
                        'total_words': len(label_words),
                        'unique_words': len(set(label_words)),
                        'most_common': label_freq.most_common(10)
                    }
            
            vocab_stats['label_vocabulary'] = label_vocab
            vocab_analysis[dataset_name] = vocab_stats
        
        self.analysis_results['vocabulary'] = vocab_analysis
        return vocab_analysis
    
    def pattern_analysis(self):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ"""
        print("\nğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ...")
        
        patterns = {}
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú©Ù„Ø§Ø³
        cooking_patterns = ['Ù¾Ø®Øª', 'Ø¢Ù…Ø§Ø¯Ù‡', 'Ø¯Ø±Ø³Øª', 'Ø³Ø³', 'ØºØ°Ø§', 'Ø´Ø§Ù…', 'Ø®ÙˆØ´Ù…Ø²Ù‡', 'Ù…Ø§Ù‡Ø±']
        tech_patterns = ['Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±', 'Ø§Ù¾', 'Ø§Ø¬Ø±Ø§', 'Ø§Ø´Ú©Ø§Ù„Ø²Ø¯Ø§ÛŒÛŒ', 'Ø¢Ù…Ø§Ø¯Ù‡']
        
        for dataset_name, df in self.datasets.items():
            pattern_stats = {}
            
            # Ø´Ù…Ø§Ø±Ø´ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ø´Ù¾Ø²ÛŒ
            cooking_count = df['text'].str.contains('|'.join(cooking_patterns)).sum()
            tech_count = df['text'].str.contains('|'.join(tech_patterns)).sum()
            
            pattern_stats['cooking_mentions'] = cooking_count
            pattern_stats['tech_mentions'] = tech_count
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
            label_patterns = {}
            for label in [0, 1]:
                label_df = df[df['label'] == label]
                label_cooking = label_df['text'].str.contains('|'.join(cooking_patterns)).sum()
                label_tech = label_df['text'].str.contains('|'.join(tech_patterns)).sum()
                
                label_patterns[label] = {
                    'cooking_patterns': label_cooking,
                    'tech_patterns': label_tech,
                    'total_samples': len(label_df)
                }
            
            pattern_stats['by_label'] = label_patterns
            patterns[dataset_name] = pattern_stats
        
        self.analysis_results['patterns'] = patterns
        return patterns
    
    def create_visualizations(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ"""
        print("\nğŸ“ˆ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§...")
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
        viz_dir = "analysis_visualizations"
        os.makedirs(viz_dir, exist_ok=True)
        
        # 1. ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
        self._plot_label_distribution(viz_dir)
        
        # 2. ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ†
        self._plot_text_length_distribution(viz_dir)
        
        # 3. ØªÙˆØ²ÛŒØ¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª
        self._plot_word_count_distribution(viz_dir)
        
        # 4. Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±
        self._plot_word_frequency(viz_dir)
        
        # 5. Ø§Ø¨Ø± Ú©Ù„Ù…Ø§Øª
        self._create_wordclouds(viz_dir)
        
        # 6. Ù…Ù‚Ø§ÛŒØ³Ù‡ dataset Ù‡Ø§
        self._plot_dataset_comparison(viz_dir)
        
        # 7. ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        self._plot_pattern_analysis(viz_dir)
        
        print(f"âœ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ {viz_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    
    def _plot_label_distribution(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        for i, (dataset_name, df) in enumerate(self.datasets.items()):
            label_counts = df['label'].value_counts()
            
            # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ
            axes[i].pie(label_counts.values, labels=['Ú©Ù„Ø§Ø³ 0', 'Ú©Ù„Ø§Ø³ 1'], 
                       autopct='%1.1f%%', startangle=90)
            axes[i].set_title(f'ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ - {dataset_name}')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'label_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_text_length_distribution(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ†"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, (dataset_name, df) in enumerate(self.datasets.items()):
            # Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ø·ÙˆÙ„ Ù…ØªÙ†
            axes[i].hist(df['text_length'], bins=20, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ† - {dataset_name}')
            axes[i].set_xlabel('Ø·ÙˆÙ„ Ù…ØªÙ† (Ú©Ø§Ø±Ø§Ú©ØªØ±)')
            axes[i].set_ylabel('ÙØ±Ø§ÙˆØ§Ù†ÛŒ')
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ
        if len(self.datasets) > 1:
            for dataset_name, df in self.datasets.items():
                axes[3].hist(df['text_length'], alpha=0.5, label=dataset_name, bins=20)
            axes[3].set_title('Ù…Ù‚Ø§ÛŒØ³Ù‡ ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ†')
            axes[3].set_xlabel('Ø·ÙˆÙ„ Ù…ØªÙ† (Ú©Ø§Ø±Ø§Ú©ØªØ±)')
            axes[3].set_ylabel('ÙØ±Ø§ÙˆØ§Ù†ÛŒ')
            axes[3].legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'text_length_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_word_count_distribution(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª"""
        fig, axes = plt.subplots(1, len(self.datasets), figsize=(5*len(self.datasets), 5))
        if len(self.datasets) == 1:
            axes = [axes]
        
        for i, (dataset_name, df) in enumerate(self.datasets.items()):
            # Box plot Ø¨Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
            data_by_label = [df[df['label'] == label]['word_count'].values for label in [0, 1]]
            axes[i].boxplot(data_by_label, labels=['Ú©Ù„Ø§Ø³ 0', 'Ú©Ù„Ø§Ø³ 1'])
            axes[i].set_title(f'ØªÙˆØ²ÛŒØ¹ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª - {dataset_name}')
            axes[i].set_ylabel('ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'word_count_distribution.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_word_frequency(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª"""
        for dataset_name, vocab_data in self.analysis_results['vocabulary'].items():
            words, frequencies = zip(*vocab_data['most_common_words'][:15])
            
            plt.figure(figsize=(12, 6))
            bars = plt.bar(range(len(words)), frequencies)
            plt.xticks(range(len(words)), words, rotation=45)
            plt.title(f'Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø± - {dataset_name}')
            plt.ylabel('ÙØ±Ø§ÙˆØ§Ù†ÛŒ')
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±
            for bar, freq in zip(bars, frequencies):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                        str(freq), ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(os.path.join(viz_dir, f'word_frequency_{dataset_name}.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
    
    def _create_wordclouds(self, viz_dir):
        """Ø§ÛŒØ¬Ø§Ø¯ Ø§Ø¨Ø± Ú©Ù„Ù…Ø§Øª"""
        for dataset_name, df in self.datasets.items():
            # Ø§Ø¨Ø± Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒ
            all_text = ' '.join(df['text'])
            processed_text = self.preprocess_text(all_text)
            
            if processed_text:
                wordcloud = WordCloud(width=800, height=400, 
                                     background_color='white',
                                     max_words=100,
                                     font_path='tahoma.ttf' if os.path.exists('tahoma.ttf') else None).generate(processed_text)
                
                plt.figure(figsize=(10, 5))
                plt.imshow(wordcloud, interpolation='bilinear')
                plt.axis('off')
                plt.title(f'Ø§Ø¨Ø± Ú©Ù„Ù…Ø§Øª - {dataset_name}')
                plt.savefig(os.path.join(viz_dir, f'wordcloud_{dataset_name}.png'), 
                           dpi=300, bbox_inches='tight')
                plt.close()
            
            # Ø§Ø¨Ø± Ú©Ù„Ù…Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
            for label in [0, 1]:
                label_text = ' '.join(df[df['label'] == label]['text'])
                processed_text = self.preprocess_text(label_text)
                
                if processed_text:
                    wordcloud = WordCloud(width=800, height=400,
                                         background_color='white',
                                         max_words=50,
                                         font_path='tahoma.ttf' if os.path.exists('tahoma.ttf') else None).generate(processed_text)
                    
                    plt.figure(figsize=(10, 5))
                    plt.imshow(wordcloud, interpolation='bilinear')
                    plt.axis('off')
                    plt.title(f'Ø§Ø¨Ø± Ú©Ù„Ù…Ø§Øª - {dataset_name} - Ú©Ù„Ø§Ø³ {label}')
                    plt.savefig(os.path.join(viz_dir, f'wordcloud_{dataset_name}_class_{label}.png'), 
                               dpi=300, bbox_inches='tight')
                    plt.close()
    
    def _plot_dataset_comparison(self, viz_dir):
        """Ù…Ù‚Ø§ÛŒØ³Ù‡ dataset Ù‡Ø§"""
        if len(self.datasets) <= 1:
            return
        
        # Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        stats_data = []
        for dataset_name, stats in self.analysis_results['basic_stats'].items():
            stats_data.append({
                'Dataset': dataset_name,
                'Total Samples': stats['total_samples'],
                'Avg Text Length': stats['text_length']['mean'],
                'Avg Word Count': stats['word_count']['mean'],
                'Vocabulary Size': self.analysis_results['vocabulary'][dataset_name]['unique_words']
            })
        
        df_stats = pd.DataFrame(stats_data)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§
        axes[0,0].bar(df_stats['Dataset'], df_stats['Total Samples'])
        axes[0,0].set_title('ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§')
        axes[0,0].set_ylabel('ØªØ¹Ø¯Ø§Ø¯')
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ù…ØªÙ†
        axes[0,1].bar(df_stats['Dataset'], df_stats['Avg Text Length'])
        axes[0,1].set_title('Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ù…ØªÙ†')
        axes[0,1].set_ylabel('Ú©Ø§Ø±Ø§Ú©ØªØ±')
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª
        axes[1,0].bar(df_stats['Dataset'], df_stats['Avg Word Count'])
        axes[1,0].set_title('Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª')
        axes[1,0].set_ylabel('Ú©Ù„Ù…Ù‡')
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†
        axes[1,1].bar(df_stats['Dataset'], df_stats['Vocabulary Size'])
        axes[1,1].set_title('Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†')
        axes[1,1].set_ylabel('Ú©Ù„Ù…Ù‡ Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯')
        
        plt.tight_layout()
        plt.savefig(os.path.join(viz_dir, 'dataset_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def _plot_pattern_analysis(self, viz_dir):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§"""
        for dataset_name, pattern_data in self.analysis_results['patterns'].items():
            labels = ['Ú©Ù„Ø§Ø³ 0', 'Ú©Ù„Ø§Ø³ 1']
            cooking_counts = [pattern_data['by_label'][0]['cooking_patterns'],
                             pattern_data['by_label'][1]['cooking_patterns']]
            tech_counts = [pattern_data['by_label'][0]['tech_patterns'],
                          pattern_data['by_label'][1]['tech_patterns']]
            
            x = np.arange(len(labels))
            width = 0.35
            
            fig, ax = plt.subplots(figsize=(10, 6))
            
            bars1 = ax.bar(x - width/2, cooking_counts, width, label='Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ø´Ù¾Ø²ÛŒ')
            bars2 = ax.bar(x + width/2, tech_counts, width, label='Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙÙ†Ø§ÙˆØ±ÛŒ')
            
            ax.set_xlabel('Ú©Ù„Ø§Ø³')
            ax.set_ylabel('ØªØ¹Ø¯Ø§Ø¯')
            ax.set_title(f'ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ - {dataset_name}')
            ax.set_xticks(x)
            ax.set_xticklabels(labels)
            ax.legend()
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ±
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height,
                           f'{int(height)}', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(os.path.join(viz_dir, f'pattern_analysis_{dataset_name}.png'), 
                       dpi=300, bbox_inches='tight')
            plt.close()
    
    def create_interactive_plots(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø¨Ø§ Plotly"""
        print("\nğŸ¯ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ...")
        
        viz_dir = "interactive_visualizations"
        os.makedirs(viz_dir, exist_ok=True)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ†
        self._create_interactive_length_plot(viz_dir)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ dataset Ù‡Ø§
        self._create_interactive_comparison(viz_dir)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª
        self._create_interactive_word_freq(viz_dir)
        
        print(f"âœ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ Ø¯Ø± Ù¾ÙˆØ´Ù‡ {viz_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    
    def _create_interactive_length_plot(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ†"""
        fig = make_subplots(rows=1, cols=len(self.datasets),
                           subplot_titles=list(self.datasets.keys()))
        
        for i, (dataset_name, df) in enumerate(self.datasets.items(), 1):
            fig.add_trace(
                go.Histogram(x=df['text_length'], name=f'{dataset_name}',
                           nbinsx=20, opacity=0.7),
                row=1, col=i
            )
        
        fig.update_layout(title_text="ØªÙˆØ²ÛŒØ¹ Ø·ÙˆÙ„ Ù…ØªÙ† - Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ",
                         showlegend=True, height=500)
        
        pyo.plot(fig, filename=os.path.join(viz_dir, 'interactive_text_length.html'), 
                auto_open=False)
    
    def _create_interactive_comparison(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ dataset Ù‡Ø§"""
        if len(self.datasets) <= 1:
            return
        
        stats_data = []
        for dataset_name, stats in self.analysis_results['basic_stats'].items():
            stats_data.append({
                'Dataset': dataset_name,
                'Total_Samples': stats['total_samples'],
                'Avg_Text_Length': round(stats['text_length']['mean'], 2),
                'Avg_Word_Count': round(stats['word_count']['mean'], 2),
                'Unique_Words': self.analysis_results['vocabulary'][dataset_name]['unique_words']
            })
        
        df_stats = pd.DataFrame(stats_data)
        
        # Ù†Ù…ÙˆØ¯Ø§Ø± radar
        fig = go.Figure()
        
        for _, row in df_stats.iterrows():
            fig.add_trace(go.Scatterpolar(
                r=[row['Total_Samples']/max(df_stats['Total_Samples'])*100,
                   row['Avg_Text_Length']/max(df_stats['Avg_Text_Length'])*100,
                   row['Avg_Word_Count']/max(df_stats['Avg_Word_Count'])*100,
                   row['Unique_Words']/max(df_stats['Unique_Words'])*100],
                theta=['ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§', 'Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ù…ØªÙ†', 'Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª', 'ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ù†Ø­ØµØ±'],
                fill='toself',
                name=row['Dataset']
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100]
                )),
            showlegend=True,
            title="Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ø§Ù…Ø¹ Dataset Ù‡Ø§"
        )
        
        pyo.plot(fig, filename=os.path.join(viz_dir, 'interactive_comparison.html'), 
                auto_open=False)
    
    def _create_interactive_word_freq(self, viz_dir):
        """Ù†Ù…ÙˆØ¯Ø§Ø± ØªØ¹Ø§Ù…Ù„ÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª"""
        for dataset_name, vocab_data in self.analysis_results['vocabulary'].items():
            words, frequencies = zip(*vocab_data['most_common_words'][:20])
            
            fig = go.Figure(data=[
                go.Bar(x=list(words), y=list(frequencies),
                      text=list(frequencies), textposition='auto')
            ])
            
            fig.update_layout(
                title=f'Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø± - {dataset_name}',
                xaxis_title='Ú©Ù„Ù…Ø§Øª',
                yaxis_title='ÙØ±Ø§ÙˆØ§Ù†ÛŒ',
                height=600
            )
            
            pyo.plot(fig, filename=os.path.join(viz_dir, f'interactive_word_freq_{dataset_name}.html'), 
                    auto_open=False)
    
    def generate_comprehensive_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹"""
        print("\nğŸ“‹ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹...")
        
        report = {
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'datasets_analyzed': list(self.datasets.keys()),
                'total_samples': sum(len(df) for df in self.datasets.values())
            },
            'basic_statistics': self.analysis_results.get('basic_stats', {}),
            'vocabulary_analysis': self.analysis_results.get('vocabulary', {}),
            'pattern_analysis': self.analysis_results.get('patterns', {}),
            'insights': self._generate_insights()
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ ÙØ±Ù…Øª JSON
        with open('comprehensive_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ
        self._save_text_report(report)
        
        print("âœ… Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
        print("   - comprehensive_analysis_report.json")
        print("   - analysis_report.txt")
        
        return report
    
    def _generate_insights(self):
        """ØªÙˆÙ„ÛŒØ¯ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ"""
        insights = []
        
        # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        for dataset_name, stats in self.analysis_results['basic_stats'].items():
            label_dist = stats['label_percentage']
            if abs(label_dist.get(0, 0) - label_dist.get(1, 0)) > 20:
                insights.append(f"â— Ø¯Ø± dataset {dataset_name} Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯")
            else:
                insights.append(f"âœ… Ø¯Ø± dataset {dataset_name} Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ù…ØªØ¹Ø§Ø¯Ù„ Ù‡Ø³ØªÙ†Ø¯")
        
        # ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†
        for dataset_name, vocab in self.analysis_results['vocabulary'].items():
            richness = vocab['vocabulary_richness']
            if richness > 0.7:
                insights.append(f"ğŸ“š dataset {dataset_name} Ø¯Ø§Ø±Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† ØºÙ†ÛŒ Ø§Ø³Øª (ØªÙ†ÙˆØ¹: {richness:.2f})")
            elif richness < 0.3:
                insights.append(f"ğŸ“– dataset {dataset_name} Ø¯Ø§Ø±Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø­Ø¯ÙˆØ¯ Ø§Ø³Øª (ØªÙ†ÙˆØ¹: {richness:.2f})")
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        for dataset_name, patterns in self.analysis_results['patterns'].items():
            cooking_in_class1 = patterns['by_label'][1]['cooking_patterns']
            tech_in_class0 = patterns['by_label'][0]['tech_patterns']
            
            if cooking_in_class1 > tech_in_class0:
                insights.append(f"ğŸ³ Ø¯Ø± dataset {dataset_name}: Ú©Ù„Ø§Ø³ 1 Ø¨ÛŒØ´ØªØ± Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¢Ø´Ù¾Ø²ÛŒ Ø§Ø³Øª")
            else:
                insights.append(f"ğŸ’» Ø¯Ø± dataset {dataset_name}: Ú©Ù„Ø§Ø³ 0 Ø¨ÛŒØ´ØªØ± Ù…Ø±ØªØ¨Ø· Ø¨Ø§ ÙÙ†Ø§ÙˆØ±ÛŒ Ø§Ø³Øª")
        
        return insights
    
    def _save_text_report(self, report):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ"""
        with open('analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ\n")
            f.write("Comprehensive Persian Data Analysis Report\n")
            f.write("=" * 60 + "\n\n")
            
            f.write(f"ğŸ“… ØªØ§Ø±ÛŒØ® ØªØ­Ù„ÛŒÙ„: {report['metadata']['analysis_date']}\n")
            f.write(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {report['metadata']['total_samples']}\n")
            f.write(f"ğŸ“ Dataset Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡: {', '.join(report['metadata']['datasets_analyzed'])}\n\n")
            
            # Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡
            f.write("ğŸ”¢ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡:\n")
            f.write("-" * 30 + "\n")
            for dataset, stats in report['basic_statistics'].items():
                f.write(f"\n{dataset}:\n")
                f.write(f"  â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {stats['total_samples']}\n")
                f.write(f"  â€¢ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {stats['label_distribution']}\n")
                f.write(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ù…ØªÙ†: {stats['text_length']['mean']:.1f} Ú©Ø§Ø±Ø§Ú©ØªØ±\n")
                f.write(f"  â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {stats['word_count']['mean']:.1f}\n")
            
            # ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†
            f.write(f"\n\nğŸ“š ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†:\n")
            f.write("-" * 30 + "\n")
            for dataset, vocab in report['vocabulary_analysis'].items():
                f.write(f"\n{dataset}:\n")
                f.write(f"  â€¢ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {vocab['total_words']}\n")
                f.write(f"  â€¢ Ú©Ù„Ù…Ø§Øª Ù…Ù†Ø­ØµØ±: {vocab['unique_words']}\n")
                f.write(f"  â€¢ ØºÙ†Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù†: {vocab['vocabulary_richness']:.3f}\n")
                f.write(f"  â€¢ Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±: {[word for word, _ in vocab['most_common_words'][:5]]}\n")
            
            # Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§
            f.write(f"\n\nğŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:\n")
            f.write("-" * 30 + "\n")
            for insight in report['insights']:
                f.write(f"  {insight}\n")
    
    def run_complete_analysis(self):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ")
        print("=" * 50)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.load_data()
        
        if not self.datasets:
            print("âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return
        
        # ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        self.basic_statistics()
        self.analyze_vocabulary()
        self.pattern_analysis()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§
        self.create_visualizations()
        self.create_interactive_plots()
        
        # ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´
        self.generate_comprehensive_report()
        
        print("\n" + "=" * 50)
        print("âœ… ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
        print("\nÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
        print("ğŸ“ analysis_visualizations/ - Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø§Ø³ØªØ§ØªÛŒÚ©")
        print("ğŸ“ interactive_visualizations/ - Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ¹Ø§Ù…Ù„ÛŒ")
        print("ğŸ“„ comprehensive_analysis_report.json - Ú¯Ø²Ø§Ø±Ø´ JSON")
        print("ğŸ“„ analysis_report.txt - Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    analyzer = PersianDataAnalyzer()
    analyzer.run_complete_analysis()


if __name__ == "__main__":
    main()
