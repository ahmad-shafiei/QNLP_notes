#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
Simple Persian Data Analysis Script

Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ
Simple analysis script without external dependencies
"""

import os
import re
import json
from collections import Counter, defaultdict
from datetime import datetime
import math

class SimplePersianAnalyzer:
    def __init__(self, data_directory="."):
        """
        Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø¯Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
        Simple Persian Data Analyzer Class
        """
        self.data_directory = data_directory
        self.datasets = {}
        self.analysis_results = {}
        self.persian_stopwords = {
            'Ø±Ø§', 'Ø¨Ù‡', 'Ø§Ø²', 'Ø¯Ø±', 'Ø¨Ø§', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ùˆ', 'ÛŒØ§', 'ØªØ§', 
            'Ø¨Ø±Ø§ÛŒ', 'Ø±ÙˆÛŒ', 'Ø²ÛŒØ±', 'Ú©Ù†Ø§Ø±', 'Ù¾ÛŒØ´', 'Ù†Ø²Ø¯', 'Ù…ÛŒØ§Ù†', 'Ø¨ÛŒÙ†'
        }
        
    def load_data(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ"""
        print("ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§...")
        
        file_patterns = {
            'train': 'mc_train_data.txt',
            'dev': 'mc_dev_data.txt', 
            'test': 'mc_test_data.txt'
        }
        
        for dataset_name, filename in file_patterns.items():
            filepath = os.path.join(self.data_directory, filename)
            if os.path.exists(filepath):
                data = []
                with open(filepath, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            parts = line.split(' ', 1)
                            if len(parts) == 2:
                                label = int(parts[0])
                                text = parts[1]
                                data.append({'label': label, 'text': text})
                
                self.datasets[dataset_name] = data
                print(f"âœ… {dataset_name}: {len(data)} Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
            else:
                print(f"âŒ ÙØ§ÛŒÙ„ {filename} ÛŒØ§ÙØª Ù†Ø´Ø¯")
    
    def preprocess_text(self, text):
        """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
        # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ
        text = re.sub(r'[^\u0600-\u06FF\s]', '', text)
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§
        text = re.sub(r'\s+', ' ', text)
        # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§
        text = text.strip()
        return text
    
    def extract_words(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ø§Ø² Ù…ØªÙ†"""
        text = self.preprocess_text(text)
        words = text.split()
        # Ø­Ø°Ù stop words
        words = [word for word in words if word not in self.persian_stopwords]
        return words
    
    def calculate_statistics(self, values):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡"""
        if not values:
            return {'mean': 0, 'median': 0, 'std': 0, 'min': 0, 'max': 0}
        
        sorted_values = sorted(values)
        n = len(values)
        mean = sum(values) / n
        median = sorted_values[n//2] if n % 2 == 1 else (sorted_values[n//2-1] + sorted_values[n//2]) / 2
        variance = sum((x - mean) ** 2 for x in values) / n
        std = math.sqrt(variance)
        
        return {
            'mean': round(mean, 2),
            'median': median,
            'std': round(std, 2),
            'min': min(values),
            'max': max(values)
        }
    
    def basic_statistics(self):
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡"""
        print("\nğŸ“Š Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡...")
        
        stats = {}
        for dataset_name, data in self.datasets.items():
            dataset_stats = {}
            
            # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
            labels = [item['label'] for item in data]
            label_counts = Counter(labels)
            total = len(data)
            
            dataset_stats['total_samples'] = total
            dataset_stats['label_distribution'] = dict(label_counts)
            dataset_stats['label_percentage'] = {
                k: round(v * 100.0 / total, 1) for k, v in label_counts.items()
            }
            
            # Ø¢Ù…Ø§Ø± Ù…ØªÙ†
            text_lengths = [len(item['text']) for item in data]
            word_counts = [len(self.extract_words(item['text'])) for item in data]
            
            dataset_stats['text_length'] = self.calculate_statistics(text_lengths)
            dataset_stats['word_count'] = self.calculate_statistics(word_counts)
            
            stats[dataset_name] = dataset_stats
        
        self.analysis_results['basic_stats'] = stats
        return stats
    
    def analyze_vocabulary(self):
        """ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†"""
        print("\nğŸ“š Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†...")
        
        vocab_analysis = {}
        
        for dataset_name, data in self.datasets.items():
            all_words = []
            words_by_label = {0: [], 1: []}
            
            for item in data:
                words = self.extract_words(item['text'])
                all_words.extend(words)
                words_by_label[item['label']].extend(words)
            
            vocab_stats = {}
            vocab_stats['total_words'] = len(all_words)
            vocab_stats['unique_words'] = len(set(all_words))
            vocab_stats['vocabulary_richness'] = round(
                len(set(all_words)) / len(all_words) if all_words else 0, 3
            )
            
            # Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±
            word_freq = Counter(all_words)
            vocab_stats['most_common_words'] = word_freq.most_common(20)
            
            # ØªØ­Ù„ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
            label_vocab = {}
            for label in [0, 1]:
                label_words = words_by_label[label]
                if label_words:
                    label_freq = Counter(label_words)
                    label_vocab[label] = {
                        'total_words': len(label_words),
                        'unique_words': len(set(label_words)),
                        'most_common': label_freq.most_common(10)
                    }
            
            vocab_stats['label_vocabulary'] = label_vocab
            vocab_analysis[dataset_name] = vocab_stats
        
        self.analysis_results['vocabulary'] = vocab_analysis
        return vocab_analysis
    
    def pattern_analysis(self):
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ"""
        print("\nğŸ” Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ...")
        
        patterns = {}
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú©Ù„Ø§Ø³
        cooking_patterns = ['Ù¾Ø®Øª', 'Ø¢Ù…Ø§Ø¯Ù‡', 'Ø¯Ø±Ø³Øª', 'Ø³Ø³', 'ØºØ°Ø§', 'Ø´Ø§Ù…', 'Ø®ÙˆØ´Ù…Ø²Ù‡', 'Ù…Ø§Ù‡Ø±']
        tech_patterns = ['Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±', 'Ø§Ù¾', 'Ø§Ø¬Ø±Ø§', 'Ø§Ø´Ú©Ø§Ù„Ø²Ø¯Ø§ÛŒÛŒ']
        
        for dataset_name, data in self.datasets.items():
            pattern_stats = {}
            
            # Ø´Ù…Ø§Ø±Ø´ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ø´Ù¾Ø²ÛŒ Ùˆ ÙÙ†Ø§ÙˆØ±ÛŒ
            cooking_count = 0
            tech_count = 0
            
            for item in data:
                text = item['text']
                if any(pattern in text for pattern in cooking_patterns):
                    cooking_count += 1
                if any(pattern in text for pattern in tech_patterns):
                    tech_count += 1
            
            pattern_stats['cooking_mentions'] = cooking_count
            pattern_stats['tech_mentions'] = tech_count
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨
            label_patterns = {}
            for label in [0, 1]:
                label_data = [item for item in data if item['label'] == label]
                label_cooking = sum(1 for item in label_data 
                                  if any(pattern in item['text'] for pattern in cooking_patterns))
                label_tech = sum(1 for item in label_data 
                               if any(pattern in item['text'] for pattern in tech_patterns))
                
                label_patterns[label] = {
                    'cooking_patterns': label_cooking,
                    'tech_patterns': label_tech,
                    'total_samples': len(label_data)
                }
            
            pattern_stats['by_label'] = label_patterns
            patterns[dataset_name] = pattern_stats
        
        self.analysis_results['patterns'] = patterns
        return patterns
    
    def create_ascii_charts(self):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ASCII Ø³Ø§Ø¯Ù‡"""
        print("\nğŸ“ˆ Ø¯Ø± Ø­Ø§Ù„ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ...")
        
        reports_dir = "analysis_reports"
        os.makedirs(reports_dir, exist_ok=True)
        
        with open(os.path.join(reports_dir, 'ascii_charts.txt'), 'w', encoding='utf-8') as f:
            f.write("Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ\n")
            f.write("=" * 50 + "\n\n")
            
            # Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
            f.write("1. ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§:\n")
            f.write("-" * 30 + "\n")
            
            for dataset_name, stats in self.analysis_results['basic_stats'].items():
                f.write(f"\n{dataset_name}:\n")
                label_dist = stats['label_distribution']
                total = stats['total_samples']
                
                for label, count in label_dist.items():
                    percentage = count * 100 / total
                    bar_length = int(percentage / 2)  # Scale for display
                    bar = "â–ˆ" * bar_length
                    f.write(f"  Ú©Ù„Ø§Ø³ {label}: {bar} {count} ({percentage:.1f}%)\n")
            
            # Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±
            f.write(f"\n\n2. Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±:\n")
            f.write("-" * 30 + "\n")
            
            for dataset_name, vocab in self.analysis_results['vocabulary'].items():
                f.write(f"\n{dataset_name}:\n")
                max_freq = vocab['most_common_words'][0][1] if vocab['most_common_words'] else 1
                
                for word, freq in vocab['most_common_words'][:10]:
                    bar_length = int(freq * 20 / max_freq)  # Scale to max 20 chars
                    bar = "â–“" * bar_length
                    f.write(f"  {word:>10}: {bar} {freq}\n")
            
            # Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§
            f.write(f"\n\n3. ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§:\n")
            f.write("-" * 30 + "\n")
            
            for dataset_name, patterns in self.analysis_results['patterns'].items():
                f.write(f"\n{dataset_name}:\n")
                
                # Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ù„Ú¯ÙˆÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³
                for label in [0, 1]:
                    f.write(f"  Ú©Ù„Ø§Ø³ {label}:\n")
                    cooking = patterns['by_label'][label]['cooking_patterns']
                    tech = patterns['by_label'][label]['tech_patterns']
                    total_label = patterns['by_label'][label]['total_samples']
                    
                    if total_label > 0:
                        cooking_pct = cooking * 100 / total_label
                        tech_pct = tech * 100 / total_label
                        
                        cooking_bar = "ğŸ³" * int(cooking_pct / 10)
                        tech_bar = "ğŸ’»" * int(tech_pct / 10)
                        
                        f.write(f"    Ø¢Ø´Ù¾Ø²ÛŒ:  {cooking_bar} {cooking} ({cooking_pct:.1f}%)\n")
                        f.write(f"    ÙÙ†Ø§ÙˆØ±ÛŒ: {tech_bar} {tech} ({tech_pct:.1f}%)\n")
        
        print(f"âœ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¯Ø± {reports_dir}/ascii_charts.txt Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    
    def generate_insights(self):
        """ØªÙˆÙ„ÛŒØ¯ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ"""
        insights = []
        
        # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
        for dataset_name, stats in self.analysis_results['basic_stats'].items():
            label_dist = stats['label_percentage']
            if abs(label_dist.get(0, 0) - label_dist.get(1, 0)) > 20:
                insights.append(f"â— Ø¯Ø± dataset {dataset_name} Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯")
            else:
                insights.append(f"âœ… Ø¯Ø± dataset {dataset_name} Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ù…ØªØ¹Ø§Ø¯Ù„ Ù‡Ø³ØªÙ†Ø¯")
        
        # ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†
        for dataset_name, vocab in self.analysis_results['vocabulary'].items():
            richness = vocab['vocabulary_richness']
            if richness > 0.7:
                insights.append(f"ğŸ“š dataset {dataset_name} Ø¯Ø§Ø±Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† ØºÙ†ÛŒ Ø§Ø³Øª (ØªÙ†ÙˆØ¹: {richness:.3f})")
            elif richness < 0.3:
                insights.append(f"ğŸ“– dataset {dataset_name} Ø¯Ø§Ø±Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø­Ø¯ÙˆØ¯ Ø§Ø³Øª (ØªÙ†ÙˆØ¹: {richness:.3f})")
        
        # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        for dataset_name, patterns in self.analysis_results['patterns'].items():
            class1_cooking = patterns['by_label'][1]['cooking_patterns']
            class1_total = patterns['by_label'][1]['total_samples']
            class0_tech = patterns['by_label'][0]['tech_patterns']
            class0_total = patterns['by_label'][0]['total_samples']
            
            if class1_total > 0 and class0_total > 0:
                cooking_ratio = class1_cooking / class1_total
                tech_ratio = class0_tech / class0_total
                
                if cooking_ratio > 0.5:
                    insights.append(f"ğŸ³ Ø¯Ø± dataset {dataset_name}: Ú©Ù„Ø§Ø³ 1 Ù‚ÙˆÛŒØ§Ù‹ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¢Ø´Ù¾Ø²ÛŒ Ø§Ø³Øª")
                if tech_ratio > 0.5:
                    insights.append(f"ğŸ’» Ø¯Ø± dataset {dataset_name}: Ú©Ù„Ø§Ø³ 0 Ù‚ÙˆÛŒØ§Ù‹ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ ÙÙ†Ø§ÙˆØ±ÛŒ Ø§Ø³Øª")
        
        return insights
    
    def generate_comprehensive_report(self):
        """ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹"""
        print("\nğŸ“‹ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹...")
        
        insights = self.generate_insights()
        
        report = {
            'metadata': {
                'analysis_date': datetime.now().isoformat(),
                'datasets_analyzed': list(self.datasets.keys()),
                'total_samples': sum(len(data) for data in self.datasets.values())
            },
            'basic_statistics': self.analysis_results.get('basic_stats', {}),
            'vocabulary_analysis': self.analysis_results.get('vocabulary', {}),
            'pattern_analysis': self.analysis_results.get('patterns', {}),
            'insights': insights
        }
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
        reports_dir = "analysis_reports"
        os.makedirs(reports_dir, exist_ok=True)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ ÙØ±Ù…Øª JSON
        with open(os.path.join(reports_dir, 'comprehensive_report.json'), 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ ÙØ§Ø±Ø³ÛŒ
        self.save_persian_report(report, reports_dir)
        
        print("âœ… Ú¯Ø²Ø§Ø±Ø´ Ø¬Ø§Ù…Ø¹ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
        print(f"   - {reports_dir}/comprehensive_report.json")
        print(f"   - {reports_dir}/persian_report.txt")
        
        return report
    
    def save_persian_report(self, report, reports_dir):
        """Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ"""
        with open(os.path.join(reports_dir, 'persian_report.txt'), 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ\n")
            f.write("Comprehensive Persian Data Analysis Report\n")
            f.write("=" * 80 + "\n\n")
            
            f.write(f"ğŸ“… ØªØ§Ø±ÛŒØ® ØªØ­Ù„ÛŒÙ„: {report['metadata']['analysis_date']}\n")
            f.write(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {report['metadata']['total_samples']}\n")
            f.write(f"ğŸ“ Dataset Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡: {', '.join(report['metadata']['datasets_analyzed'])}\n\n")
            
            # Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡
            f.write("ğŸ”¢ Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡ Dataset Ù‡Ø§:\n")
            f.write("=" * 40 + "\n")
            for dataset, stats in report['basic_statistics'].items():
                f.write(f"\nğŸ“ {dataset.upper()}:\n")
                f.write(f"{'':>4}â€¢ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {stats['total_samples']}\n")
                f.write(f"{'':>4}â€¢ ØªÙˆØ²ÛŒØ¹ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {stats['label_distribution']}\n")
                f.write(f"{'':>4}â€¢ Ø¯Ø±ØµØ¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {stats['label_percentage']}\n")
                f.write(f"{'':>4}â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø·ÙˆÙ„ Ù…ØªÙ†: {stats['text_length']['mean']} Ú©Ø§Ø±Ø§Ú©ØªØ±\n")
                f.write(f"{'':>4}â€¢ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {stats['word_count']['mean']}\n")
                f.write(f"{'':>4}â€¢ Ø¯Ø§Ù…Ù†Ù‡ Ø·ÙˆÙ„ Ù…ØªÙ†: {stats['text_length']['min']} - {stats['text_length']['max']}\n")
                f.write(f"{'':>4}â€¢ Ø¯Ø§Ù…Ù†Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {stats['word_count']['min']} - {stats['word_count']['max']}\n")
            
            # ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†
            f.write(f"\n\nğŸ“š ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†:\n")
            f.write("=" * 40 + "\n")
            for dataset, vocab in report['vocabulary_analysis'].items():
                f.write(f"\nğŸ“ {dataset.upper()}:\n")
                f.write(f"{'':>4}â€¢ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {vocab['total_words']:,}\n")
                f.write(f"{'':>4}â€¢ Ú©Ù„Ù…Ø§Øª Ù…Ù†Ø­ØµØ± Ø¨Ù‡ ÙØ±Ø¯: {vocab['unique_words']:,}\n")
                f.write(f"{'':>4}â€¢ ØºÙ†Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù†: {vocab['vocabulary_richness']}\n")
                
                f.write(f"{'':>4}â€¢ Û±Û° Ú©Ù„Ù…Ù‡ Ù¾Ø±ØªÚ©Ø±Ø§Ø±:\n")
                for i, (word, freq) in enumerate(vocab['most_common_words'][:10], 1):
                    f.write(f"{'':>8}{i:>2}. {word} ({freq} Ø¨Ø§Ø±)\n")
                
                # ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³
                f.write(f"{'':>4}â€¢ ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³:\n")
                for label, label_vocab in vocab['label_vocabulary'].items():
                    f.write(f"{'':>8}Ú©Ù„Ø§Ø³ {label}:\n")
                    f.write(f"{'':>12}â—‹ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {label_vocab['total_words']}\n")
                    f.write(f"{'':>12}â—‹ Ú©Ù„Ù…Ø§Øª Ù…Ù†Ø­ØµØ±: {label_vocab['unique_words']}\n")
                    f.write(f"{'':>12}â—‹ Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±: ")
                    top_words = [word for word, _ in label_vocab['most_common'][:5]]
                    f.write(f"{', '.join(top_words)}\n")
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            f.write(f"\n\nğŸ” ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ:\n")
            f.write("=" * 40 + "\n")
            for dataset, patterns in report['pattern_analysis'].items():
                f.write(f"\nğŸ“ {dataset.upper()}:\n")
                f.write(f"{'':>4}â€¢ Ú©Ù„ Ø§Ø´Ø§Ø±Ø§Øª Ø¢Ø´Ù¾Ø²ÛŒ: {patterns['cooking_mentions']}\n")
                f.write(f"{'':>4}â€¢ Ú©Ù„ Ø§Ø´Ø§Ø±Ø§Øª ÙÙ†Ø§ÙˆØ±ÛŒ: {patterns['tech_mentions']}\n")
                
                f.write(f"{'':>4}â€¢ ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³:\n")
                for label, label_patterns in patterns['by_label'].items():
                    total = label_patterns['total_samples']
                    cooking = label_patterns['cooking_patterns']
                    tech = label_patterns['tech_patterns']
                    
                    cooking_pct = (cooking * 100 / total) if total > 0 else 0
                    tech_pct = (tech * 100 / total) if total > 0 else 0
                    
                    f.write(f"{'':>8}Ú©Ù„Ø§Ø³ {label} ({total} Ù†Ù…ÙˆÙ†Ù‡):\n")
                    f.write(f"{'':>12}â—‹ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¢Ø´Ù¾Ø²ÛŒ: {cooking} ({cooking_pct:.1f}%)\n")
                    f.write(f"{'':>12}â—‹ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ÙÙ†Ø§ÙˆØ±ÛŒ: {tech} ({tech_pct:.1f}%)\n")
            
            # Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ
            f.write(f"\n\nğŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ Ùˆ Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:\n")
            f.write("=" * 40 + "\n")
            for i, insight in enumerate(report['insights'], 1):
                f.write(f"{i:>2}. {insight}\n")
            
            # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
            f.write(f"\n\nğŸ¯ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:\n")
            f.write("=" * 40 + "\n")
            
            # ØªØ­Ù„ÛŒÙ„ Ú©ÛŒÙÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            total_samples = report['metadata']['total_samples']
            if total_samples < 1000:
                f.write("1. ğŸ“ˆ Ø§ÙØ²Ø§ÛŒØ´ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¯Ù‚Øª Ù…Ø¯Ù„ ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n")
            
            # ØªØ­Ù„ÛŒÙ„ ØªØ¹Ø§Ø¯Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
            imbalanced_datasets = []
            for dataset, stats in report['basic_statistics'].items():
                label_dist = stats['label_percentage']
                if abs(label_dist.get(0, 0) - label_dist.get(1, 0)) > 20:
                    imbalanced_datasets.append(dataset)
            
            if imbalanced_datasets:
                f.write(f"2. âš–ï¸ ØªØ¹Ø§Ø¯Ù„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¯Ø± dataset Ù‡Ø§ÛŒ {', '.join(imbalanced_datasets)} Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§Ø¨Ø¯\n")
            
            # ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            clear_patterns = True
            for dataset, patterns in report['pattern_analysis'].items():
                class1_cooking = patterns['by_label'][1]['cooking_patterns']
                class0_tech = patterns['by_label'][0]['tech_patterns']
                class1_total = patterns['by_label'][1]['total_samples'] 
                class0_total = patterns['by_label'][0]['total_samples']
                
                if class1_total > 0 and class0_total > 0:
                    if (class1_cooking / class1_total < 0.7) or (class0_tech / class0_total < 0.7):
                        clear_patterns = False
            
            if clear_patterns:
                f.write("3. âœ… Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒØ¨Ù†Ø¯ÛŒ ÙˆØ§Ø¶Ø­ Ù‡Ø³ØªÙ†Ø¯ - Ù…Ø¯Ù„ Ø¨Ø§ÛŒØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯\n")
            else:
                f.write("3. ğŸ”„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ù…Ù„Ø§Ù‹ ÙˆØ§Ø¶Ø­ Ù†ÛŒØ³ØªÙ†Ø¯ - Ù†ÛŒØ§Ø² Ø¨Ù‡ feature engineering\n")
            
            f.write("\n" + "=" * 80 + "\n")
            f.write("Ù¾Ø§ÛŒØ§Ù† Ú¯Ø²Ø§Ø±Ø´\n")
    
    def run_complete_analysis(self):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ")
        print("=" * 60)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.load_data()
        
        if not self.datasets:
            print("âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
            return
        
        # ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        self.basic_statistics()
        self.analyze_vocabulary()
        self.pattern_analysis()
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§
        self.create_ascii_charts()
        self.generate_comprehensive_report()
        
        print("\n" + "=" * 60)
        print("âœ… ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯!")
        print("\nÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:")
        print("ğŸ“ analysis_reports/ - Ù¾ÙˆØ´Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§")
        print("  â”œâ”€â”€ comprehensive_report.json - Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ JSON")
        print("  â”œâ”€â”€ persian_report.txt - Ú¯Ø²Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ ØªÙØµÛŒÙ„ÛŒ")
        print("  â””â”€â”€ ascii_charts.txt - Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù…ØªÙ†ÛŒ")
        
        # Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        self.print_summary()
    
    def print_summary(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬"""
        print("\nğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
        print("-" * 30)
        
        total_samples = sum(len(data) for data in self.datasets.values())
        print(f"ğŸ“Š Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {total_samples}")
        
        for dataset_name, data in self.datasets.items():
            labels = [item['label'] for item in data]
            label_counts = Counter(labels)
            print(f"ğŸ“ {dataset_name}: {len(data)} Ù†Ù…ÙˆÙ†Ù‡ - {dict(label_counts)}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ú©Ù„ÛŒ
        all_words = []
        for data in self.datasets.values():
            for item in data:
                all_words.extend(self.extract_words(item['text']))
        
        if all_words:
            top_words = Counter(all_words).most_common(5)
            print(f"ğŸ” Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±: {[word for word, _ in top_words]}")


def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    analyzer = SimplePersianAnalyzer()
    analyzer.run_complete_analysis()


if __name__ == "__main__":
    main()
