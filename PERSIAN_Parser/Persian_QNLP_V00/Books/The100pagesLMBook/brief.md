# ๐ง ูุตู ุงูู: ูุจุงู ุงุฏฺฏุฑ ูุงุดู (Machine Learning Basics)

---

## 1. ููุฏููโุง ุจุฑ ููุด ูุตููุน ู ุงุฏฺฏุฑ ูุงุดู

- **ููุด ูุตููุน (AI)** ููููู ุงุณุช ฺฉู ุฏุฑ ุณุงู 1955 ุชูุณุท **John McCarthy** ูุทุฑุญ ุดุฏุ ูุฏู ุขู ุณุงุฎุช ูุงุดูโูุง ุจูุฏ ฺฉู ุจุชูุงููุฏ ูุงููุฏ ุงูุณุงู ุจูุฏุดูุฏุ ุงุฏ ุจฺฏุฑูุฏ ู ุงุฑุชุจุงุท ุจุฑูุฑุงุฑ ฺฉููุฏ.
- **ุฑูุฏุงุฏูุง ุชุงุฑุฎ ููู:**
  - **1956 โ Logic Theorist:** ูุฎุณุชู ุจุฑูุงูู ุงุณุชุฏูุงู ุฎูุฏฺฉุงุฑ.
  - **1958 โ Perceptron:** ุงููู ุดุจฺฉู ุนุตุจ ุขููุฒุดโูพุฐุฑ (Frank Rosenblatt).
  - **1967 โ ELIZA:** ุงููู ฺุชโุจุงุช ุฒุจุงู ุทุจุน.
  - **1959 โ Arthur Samuel:** ุชุนุฑู ุฑุณู *Machine Learning* ุจูโุนููุงู ุจุฑูุงููโููุณ ฺฉุงููพูุชุฑ ุจุฑุง ุงุฏฺฏุฑ ุงุฒ ุชุฌุฑุจู.

---

## 2. ูุฏู ุงุฏฺฏุฑ ูุงุดู (Machine Learning Model)

ฺฉ ูุฏู ุงุฏฺฏุฑ ูุงุดู ุชุงุจุน ุงุณุช ฺฉู ูุฑูุฏโูุง ุฑุง ุจู ุฎุฑูุฌ ูพุดโุจูโุดุฏู ูฺฏุงุดุช ูโฺฉูุฏ.

\[
\hat{y} = w x + b
\]

- \( x \): ูุฑูุฏ (feature)
- \( y \): ุฎุฑูุฌ ูุงูุน
- \( \hat{y} \): ุฎุฑูุฌ ูพุดโุจูโุดุฏู
- \( w, b \): ูพุงุฑุงูุชุฑูุง ูุงุจู ุงุฏฺฏุฑ ูุฏู

---

## 3. ฺูุงุฑ ฺฏุงู ุงุฏฺฏุฑ ูุงุดู (Four-Step ML Process)

| ฺฏุงู | ุชูุถุญ | ูุซุงู |
|------|--------|--------|
| **1. ฺฏุฑุฏุขูุฑ ุฏุงุฏูโูุง** | ูุฌููุนู ุฏุงุฏู \((x, y)\) ุดุงูู ูุฑูุฏ ู ุฎุฑูุฌ | (150, 200), (200, 600) |
| **2. ุชุนุฑู ูุฏู** | ุชุงุจุน ุฑุงุจุทู ุจู ูุฑูุฏ ู ุฎุฑูุฌ | \( y = w x + b \) |
| **3. ุชุนุฑู ุชุงุจุน ุฎุทุง (Loss)** | ุงูุฏุงุฒูโฺฏุฑ ุฎุทุง | \( J(w,b) = \frac{1}{n}\sum (y_i - \hat{y}_i)^2 \) |
| **4. ฺฉูููโุณุงุฒ ุฎุทุง** | ุจุง ุฑูุดโูุง ูุซู Gradient Descent | ูพุฏุง ฺฉุฑุฏู \( w^*, b^* \) |

---

## 4. ุจุฑุฏุงุฑ ูฺฺฏโูุง (Feature Vector)

ุฏุฑ ุญุงูุช ฺูุฏ ูฺฺฏุ ูุฑูุฏ ู ูุฒูโูุง ุจู ุตูุฑุช ุจุฑุฏุงุฑ ุฏุฑ ูุธุฑ ฺฏุฑูุชู ูโุดููุฏ:

\[
\hat{y} = \mathbf{w}^\top \mathbf{x} + b
\]

ฺฉู ุฏุฑ ุขู:

\[
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}, \quad
\mathbf{w} =
\begin{bmatrix}
w_1 \\
w_2
\end{bmatrix}
\]

---

## 5. ุดุจฺฉูโูุง ุนุตุจ (Neural Networks)

ฺฉ **ุดุจฺฉู ุนุตุจ (NN)** ุงุฒ ููุฑููโูุง ุชุดฺฉู ุดุฏู ฺฉู ูุฑ ฺฉุฏุงู ุณฺฏูุงู ุฑุง ูพุฑุฏุงุฒุด ฺฉุฑุฏู ู ุจู ูุงู ุจุนุฏ ุงุฑุณุงู ูโฺฉููุฏ.

### ุงููุงุน ุดุจฺฉูโูุง

| ููุน ุดุจฺฉู | ุณุงุฎุชุงุฑ | ุชูุถุญ |
|-----------|----------|--------|
| **Feedforward NN (FNN)** | ุฏุงุฏูโูุง ุชููุง ุฏุฑ ฺฉ ุฌูุช ุญุฑฺฉุช ูโฺฉููุฏ | ุณุงุฏู ู ูพุงูโุง |
| **Multilayer Perceptron (MLP)** | ฺูุฏู ูุงู Fully Connected | ูพุฑฺฉุงุฑุจุฑุฏ ุฏุฑ ุทุจููโุจูุฏ |
| **Recurrent NN (RNN)** | ุฏุงุฑุง ุญููู ุจุงุฒุฎูุฑุฏ ุจุฑุง ุฏุงุฏูโูุง ุชุฑุชุจ | ููุงุณุจ ุจุฑุง ูุชู ู ุตูุช |
| **Convolutional NN (CNN)** | ุงุณุชูุงุฏู ุงุฒ ููุชุฑูุง ฺฉุงููููุดู | ููุงุณุจ ุจุฑุง ุชุตูุฑ ู ูุชู |

---

## 6. ุชุงุจุน ูุนุงูโุณุงุฒ (Activation Function)

ุชูุงุจุน ฺฉู ุฎุฑูุฌ ูุฑ ููุฑูู ุฑุง ุจู ูุงุญู ุฎุงุต ูฺฏุงุดุช ูโฺฉููุฏ ู ุบุฑุฎุทโุจูุฏู ุฑุง ูุงุฑุฏ ูุฏู ูโุณุงุฒูุฏ.

| ุชุงุจุน | ุชุนุฑู | ูฺฺฏ |
|--------|--------|--------|
| **Sigmoid** | \( \sigma(x) = \frac{1}{1+e^{-x}} \) | ุฎุฑูุฌ ุฏุฑ ุจุงุฒู (0,1) |
| **tanh** | \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \) | ุฎุฑูุฌ ุฏุฑ ุจุงุฒู (-1,1) |
| **ReLU** | \( \text{ReLU}(x) = \max(0, x) \) | ุณุฑุนุ ุณุงุฏูุ ูุญุจูุจ ุฏุฑ ุดุจฺฉูโูุง ุนูู |

> ๐ก **ReLU** ุงุฒ ุณุงู 2012 ุจุงุนุซ ุฌูุด ุจุฒุฑฺฏ ุฏุฑ ุขููุฒุด ุดุจฺฉูโูุง ุนูู ุดุฏ ฺูู ุงุฒ ูุงูพุฏุฏ ุดุฏู ฺฏุฑุงุฏุงู ุฌููฺฏุฑ ูโฺฉูุฏ.

---

## 7. ูุงุชุฑุณโูุง ู ูุญุงุณุจุงุช ุจุฑุฏุงุฑ

ุฏุฑ ุดุจฺฉูโูุง ุนุตุจุ ูุฒูโูุง ู ูุฑูุฏโูุง ูุนูููุงู ุจูโุตูุฑุช ูุงุชุฑุณ ููุงุด ุฏุงุฏู ูโุดููุฏ:

\[
\mathbf{A} =
\begin{bmatrix}
a_{11} & a_{12} & \dots & a_{1n} \\
a_{21} & a_{22} & \dots & a_{2n}
\end{bmatrix}
\]

ุฏุฑ **PyTorch**:

```python
import torch
A = torch.tensor([[1, 2], [3, 4]])
x = torch.tensor([[5], [6]])
y = torch.mm(A, x)  # matrix multiplication

```
---

## 8. ุงูฺฏูุฑุชู ูุฒูู ฺฏุฑุงุฏุงู (Gradient Descent)

ุงูฺฏูุฑุชู ูุฒูู ฺฏุฑุงุฏุงู (Gradient Descent) ฺฉ ุงุฒ ูพุงูโุงโุชุฑู ุฑูุดโูุง ุจูููโุณุงุฒ ุฏุฑ ุงุฏฺฏุฑ ูุงุดู ุงุณุช.  
ูุฏู ุขู ฺฉููู ฺฉุฑุฏู ุชุงุจุน ุฎุทุง (Loss Function) ุงุฒ ุทุฑู ุจูโุฑูุฒุฑุณุงู ุชุฏุฑุฌ ูพุงุฑุงูุชุฑูุง ูุฏู ุงุณุช.

### ูุฑููู ฺฉู:

\[
\theta := \theta - \eta \frac{\partial J(\theta)}{\partial \theta}
\]

ฺฉู ุฏุฑ ุขู:
- \( \theta \): ูพุงุฑุงูุชุฑูุง ูุฏู (ูุซู ูุฒูโูุง ู ุจุงุงุณ)
- \( J(\theta) \): ุชุงุจุน ุฎุทุง (Loss Function)
- \( \eta \): ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate)

> ูุฏู ุฏุฑ ูุฑ ฺฏุงู ุฏุฑ ุฌูุช ูุฎุงูู ฺฏุฑุงุฏุงู ุชุงุจุน ุฎุทุง ุญุฑฺฉุช ูโฺฉูุฏ ุชุง ุจู ููุฏุงุฑ ุจููู ุจุฑุณุฏ.

### ุงููุงุน Gradient Descent

| ููุน | ุชูุถุญ | ูฺฺฏ |
|------|--------|--------|
| **Batch GD** | ุงุณุชูุงุฏู ุงุฒ ฺฉู ุฏุงุฏูโูุง ุฏุฑ ูุฑ ุจูโุฑูุฒุฑุณุงู | ุฏูู ูู ฺฉูุฏ |
| **Stochastic GD (SGD)** | ุจูโุฑูุฒุฑุณุงู ุจุฑ ุงุณุงุณ ูุฑ ููููู | ุณุฑุนโุชุฑ ูู ูพุฑููุณุงู |
| **Mini-Batch GD** | ุชุฑฺฉุจ ุฏู ุฑูุด ุจุงูุง | ูุชุนุงุฏู ู ูพุฑฺฉุงุฑุจุฑุฏ ุฏุฑ PyTorch |

---

## 9. ูพุงุฏูโุณุงุฒ Logistic Regression ุฏุฑ PyTorch

ุฏุฑ PyTorch ูุฏูโูุง ุงุฏฺฏุฑ ูุงุดู ุจุง ุงุณุชูุงุฏู ุงุฒ ฺฉูุงุณโูุง `nn.Module` ุง `nn.Sequential` ุชุนุฑู ูโุดููุฏ.

### ูุซุงู ุณุงุฏู ุงุฒ Logistic Regression:

```python
import torch
import torch.nn as nn
import torch.optim as optim

# ูุฏู ุดุงูู ฺฉ ูุงู ุฎุท ู ุชุงุจุน ูุนุงูโุณุงุฒ ุณฺฏููุฏ
model = nn.Sequential(
    nn.Linear(n_inputs, n_outputs),
    nn.Sigmoid()
)

# ุชุงุจุน ุฎุทุง ู ุจูููโุณุงุฒ
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss
optimizer = optim.SGD(model.parameters(), lr=0.1)

# ุญููู ุขููุฒุด (Training Loop)
for epoch in range(100):
    optimizer.zero_grad()      # ุตูุฑ ฺฉุฑุฏู ฺฏุฑุงุฏุงูโูุง
    outputs = model(x_train)   # ูพุดโุจู
    loss = criterion(outputs, y_train)  # ูุญุงุณุจู ุฎุทุง
    loss.backward()            # ูุญุงุณุจู ฺฏุฑุงุฏุงูโูุง
    optimizer.step()           # ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง
```
