# preprocess_hazm.py
# -*- coding: utf-8 -*-
"""
preprocess_hazm.py
Hazm-based preprocessing with:
 - multiword merging (e.g. 'Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±' -> 'Ù†Ø±Ù…_Ø§ÙØ²Ø§Ø±')
 - POS tagging via Hazm
 - merging light verbs into compound verbs (e.g. 'Ø§Ø¬Ø±Ø§' + 'Ú©Ø±Ø¯' -> 'Ø§Ø¬Ø±Ø§_Ú©Ø±Ø¯' as VERB)
 - simple role labeling (subj/obj/adj/verb/ra/other)
"""

import os
from hazm import Normalizer, POSTagger, word_tokenize

# ------------------------
# Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
# ------------------------
MODEL_DIR = "resources"
POS_MODEL = os.path.join(MODEL_DIR, "pos_tagger.model")

if not os.path.exists(POS_MODEL):
    raise FileNotFoundError(f"POS model not found at {POS_MODEL}. Put pos_tagger.model in resources/")

normalizer = Normalizer()
tagger = POSTagger(model=POS_MODEL)

def process_sentence(text: str):
    """Full pipeline: normalize + tokenize + role labeling."""
    norm = normalizer.normalize(text)   # ğŸ‘ˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø±Ù…Ø§Ù„Ø§ÛŒØ²Ø± Ù‡Ø¶Ù…
    tokens = word_tokenize(norm)        # ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ù‡Ø¶Ù…
    labeled = label_roles(tokens)
    return labeled
# ------------------------
# multiword expressions (configurable)
# ------------------------
MULTIWORDS = [
    "Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±",
    "Ø§Ø´Ú©Ø§Ù„ Ø²Ø¯Ø§ÛŒÛŒ",
    "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†",
    "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ",
    "Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ",
]

def merge_multiwords(tokens, multiwords_list=None):
    """
    Merge multiword expressions in tokens.
    Keep spaces intact, do not convert to underscores.
    """
    if multiwords_list is None:
        multiwords_list = MULTIWORDS

    merged = []
    skip = 0
    i = 0
    while i < len(tokens):
        if skip > 0:
            skip -= 1
            i += 1
            continue
        matched = False
        for mw in multiwords_list:
            L = len(mw.split())
            if i + L <= len(tokens) and tokens[i:i+L] == mw.split():
                merged.append(" ".join(mw.split()))  # keep spaces
                skip = L - 1
                matched = True
                break
        if not matched:
            merged.append(tokens[i])
        i += 1
    return merged

import unicodedata

def strip_punct_tokens(tokens):
    """Remove tokens that are pure punctuation marks."""
    clean = []
    for tok in tokens:
        # Ø­Ø°Ù Ø§Ú¯Ø± Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØªÙˆÚ©Ù† Ø¬Ø²Ùˆ Punctuation Ø¨Ø§Ø´Ù†Ø¯
        if all(unicodedata.category(ch).startswith('P') for ch in tok):
            continue
        clean.append(tok)
    return clean
# ------------------------
# light verbs (Ø§ÙØ¹Ø§Ù„ Ù…Ø±Ú©Ø¨)
# ------------------------
LIGHT_VERBS = ['Ú©Ø±Ø¯', 'Ø¯Ø§Ø¯', 'Ú¯Ø±ÙØª', 'Ø²Ø¯', 'Ø´Ø¯', 'Ø§Ø³Øª', 'Ø¨ÙˆØ¯', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ù…ÛŒâ€ŒÚ©Ù†Ø¯', 'Ù…ÛŒâ€ŒØ±ÙˆØ¯']

def merge_light_verbs(tagged_tokens, light_verbs=None):
    """
    Keep compound verbs as two tokens for Lambeq wiring,
    but combine them into one string for display.
    Returns list of (token, pos)
    """
    if light_verbs is None:
        light_verbs = LIGHT_VERBS

    merged = []
    i = 0
    n = len(tagged_tokens)
    while i < n:
        w, pos = tagged_tokens[i]
        if i + 1 < n:
            nw, npos = tagged_tokens[i+1]
            if nw in light_verbs:
                # combine for display only
                # new_word_display = f"{w} {nw}"
                # append separately for Lambeq wiring
                new_word = f"{w} {nw}"
                # merged.append((w, pos))
                merged.append((new_word, 'VERB'))
                i += 2
                continue
        merged.append((w, pos))
        i += 1
    return merged

# ------------------------
# Ù†Ù‚Ø´â€ŒØ¯Ù‡ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ POS Ùˆ particle 'Ø±Ø§'
# ------------------------
def label_roles_from_tagged(tagged_tokens):
    """
    tagged_tokens: list of (word, pos) - pos are Hazm tags like 'NOUN,EZ' or 'VERB' etc.
    Returns: list of (word, role) where role in {'subj','obj','adj','verb','ra','other'}
    Rules:
      - mark 'Ø±Ø§' tokens as 'ra'
      - mark tokens with pos starting with V as 'verb'
      - mark tokens with pos starting with ADJ as 'adj'
      - find 'Ø±Ø§' and mark the noun(s) before it as obj (skip post-nominal adjectives)
      - first remaining noun -> subj
      - remaining nouns default to 'other' (or 'subj/obj' if you prefer)
    """
    words = [w for w, _ in tagged_tokens]
    pos_tags = [p for _, p in tagged_tokens]
    n = len(tagged_tokens)
    roles = ['other'] * n

    # initial marking
    for i, (w, p) in enumerate(tagged_tokens):
        up = (p or "").upper()
        if w == 'Ø±Ø§' or up.startswith('ADP'):
            roles[i] = 'ra'
        elif up.startswith('V'):
            roles[i] = 'verb'
        elif up.startswith('ADJ'):
            roles[i] = 'adj'
        else:
            roles[i] = 'other'

    # handle 'Ø±Ø§' -> mark object span before 'Ø±Ø§'
    ra_indices = [i for i, w in enumerate(words) if w == 'Ø±Ø§']
    for r in ra_indices:
        if r == 0:
            continue
        # scan left to find head (skip trailing adjectives)
        j = r - 1
        # skip adjectives to find head
        while j > 0 and ((tagged_tokens[j][1] or "").upper().startswith('ADJ')):
            j -= 1
        head = j
        # mark head..r-1
        for k in range(head, r):
            if k == head:
                roles[k] = 'obj'
            else:
                # if tag is adjective mark as adj, else mark as part-of-obj (keep 'other' or 'obj-part')
                if (tagged_tokens[k][1] or "").upper().startswith('ADJ'):
                    roles[k] = 'adj'
                else:
                    roles[k] = 'other'

    # mark first noun (not already obj) as subj
    for i, (w, p) in enumerate(tagged_tokens):
        up = (p or "").upper()
        if roles[i] == 'other' and up.startswith('N'):
            roles[i] = 'subj'
            break

    # finalize list
    result = [(tagged_tokens[i][0], roles[i]) for i in range(n)]
    return result

# ------------------------
# ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ù…Ù„Ù‡
# ------------------------
def process_sentence(text: str):
    """
    Full pipeline:
      - normalize
      - tokenize (Hazm)
      - merge multiwords (config list)
      - POS tag (Hazm)
      - merge light verbs (compound verbs)
      - role labeling (ra rule + first-noun-as-subj)
    Returns:
      list of (token, role)   # Ø³Ø§Ø²Ú¯Ø§Ø± Ø¨Ø§ Ø®Ø±ÙˆØ¬ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ preprocess.py
    """
    norm = normalizer.normalize(text)
    tokens = word_tokenize(norm)
    tokens = strip_punct_tokens(tokens)    # ğŸ‘ˆ ÙˆØ¸ÛŒÙÙ‡ Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ùˆ Ù†Ù‚Ø·Ù‡  Ø¯Ø± Ø¬Ù…Ù„Ù‡
    tokens = merge_multiwords(tokens)              # e.g. 'Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±' -> 'Ù†Ø±Ù…_Ø§ÙØ²Ø§Ø±'
    tagged = tagger.tag(tokens)                    # list of (word, pos)

    # merge light verbs into compound verbs (affects tagged list)
    merged_tagged = merge_light_verbs(tagged)

    # label roles based on merged tags
    roles = label_roles_from_tagged(merged_tagged)

    # Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ù†Ø³Ø®Ù‡ Ù‚Ø¯ÛŒÙ…ÛŒ: [(token, role), ...]
    labeled = [(w, r) for (w, _), (_, r) in zip(merged_tagged, roles)]
    return labeled

def read_data(filename, sep=" "):
    """Read dataset with labels and sentences (binary labels as [t, 1-t])."""
    labels, sentences = [], []
    with open(filename, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒØ¨Ù„ Ø§Ø² Ø¬Ù…Ù„Ù‡
            parts = line.split(sep, 1)
            if len(parts) < 2:
                continue
            label_str, sentence = parts
            t = int(label_str)
            # Ù…Ø´Ø§Ø¨Ù‡ Ù†Ø³Ø®Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø§ÛŒÙ†Ø±ÛŒ Ø¯Ùˆ Ø¨Ø¹Ø¯ÛŒ
            labels.append([t, 1 - t])
            sentences.append(sentence.strip())
    return labels, sentences
# ------------------------
# ØªØ³Øª Ø³Ø±ÛŒØ¹
# ------------------------
if __name__ == "__main__":
    tests = [
        "Ù…Ø±Ø¯ ØºØ°Ø§ÛŒ Ø®ÙˆØ´Ù…Ø²Ù‡ Ø±Ø§ Ù¾Ø®Øª",
        "Ø´Ø®Øµ Ù…Ø§Ù‡Ø± ØºØ°Ø§ Ø±Ø§ Ù¾Ø®Øª",
        "Ù…Ø±Ø¯ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ø±Ø¯",
        "Ø§Ùˆ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØª Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ø±Ø¯"
    ]
    for sent in tests:
        res = process_sentence(sent)
        print("===\nØ¬Ù…Ù„Ù‡:", sent)
        print("Tokens:", res["tokens"])
        print("POS Tags:", res["pos_tags"])
        print("Roles:", res["roles"])



# # -*- coding: utf-8 -*-
# """
# preprocess_hazm.py
# Ù†Ø³Ø®Ù‡ Ø§Ø±ØªÙ‚Ø§ÛŒØ§ÙØªÙ‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hazm + Universal Dependency Parser (Ø¯Ø± ØµÙˆØ±Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¨ÙˆØ¯Ù†)
# """

# import os
# from hazm import Normalizer, word_tokenize, POSTagger, DependencyParser

# try:
#     from hazm import DependencyParser
#     HAS_DEP_PARSER = True
# except ImportError:
#     HAS_DEP_PARSER = False

# # ------------------------ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ ------------------------
# MODEL_DIR = "resources"
# POS_TAGGER_MODEL = os.path.join(MODEL_DIR, "pos_tagger.model")
# DEP_PARSER_MODEL = os.path.join(MODEL_DIR, "universal_dependency_parser")  # Ø¨Ø§ÛŒØ¯ unzip Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù‡

# # ------------------------ Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ Hazm ------------------------
# normalizer = Normalizer()
# pos_tagger = POSTagger(model=POS_TAGGER_MODEL)

# dep_parser = None
# if HAS_DEP_PARSER and os.path.exists(DEP_PARSER_MODEL):
#     try:
#         dep_parser = DependencyParser(model=DEP_PARSER_MODEL)
#     except Exception as e:
#         print(f"[WARN] DependencyParser load failed: {e}")
#         dep_parser = None

# # ------------------------ multiwords ------------------------
# DEFAULT_MULTIWORDS = [
#     ["Ù†Ø±Ù…", "Ø§ÙØ²Ø§Ø±"],
#     ["Ù‡ÙˆØ´", "Ù…ØµÙ†ÙˆØ¹ÛŒ"],
#     ["ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", "Ù…Ø§Ø´ÛŒÙ†"],
#     ["Ø§Ø´Ú©Ø§Ù„", "Ø²Ø¯Ø§ÛŒÛŒ"],
# ]

# def merge_multiwords(tokens, multiwords_list=None):
#     if multiwords_list is None:
#         multiwords_list = DEFAULT_MULTIWORDS
#     merged, skip, i = [], 0, 0
#     while i < len(tokens):
#         if skip > 0:
#             skip -= 1
#             i += 1
#             continue
#         matched = False
#         for mw in multiwords_list:
#             L = len(mw)
#             if i + L <= len(tokens) and tokens[i:i+L] == mw:
#                 merged.append("_".join(mw))
#                 skip = L - 1
#                 matched = True
#                 break
#         if not matched:
#             merged.append(tokens[i])
#         i += 1
#     return merged

# # ------------------------ Ù†Ù‚Ø´â€ŒØ¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ POS ------------------------
# def label_roles_with_pos(tokens):
#     tagged = pos_tagger.tag(tokens)
#     roles = []
#     for i, (tok, pos) in enumerate(tagged):
#         role = "other"
#         if pos.startswith("N"):
#             role = "subj" if i == 0 else "obj"
#         elif pos.startswith("V"):
#             role = "verb"
#         elif pos.startswith("ADJ"):
#             role = "adj"
#         roles.append((tok, role))
#     return roles

# # ------------------------ Ù†Ù‚Ø´â€ŒØ¯Ù‡ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ DependencyParser ------------------------
# def label_roles_with_dep(tokens):
#     """Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ù†Ø­ÙˆÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù‚Ø´â€ŒØ¯Ù‡ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±"""
#     tagged = pos_tagger.tag(tokens)  # Ù†ÛŒØ§Ø² parser Ø¨Ù‡ POS
#     tree = dep_parser.parse(tagged)
#     roles = []
#     for node in tree.nodes.values():
#         if node["word"] is None:
#             continue
#         rel = node["rel"]  # Ù†ÙˆØ¹ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ù…Ø«Ù„ nsubj, obj, amod
#         role = "other"
#         if rel == "nsubj":
#             role = "subj"
#         elif rel in ["obj", "dobj"]:
#             role = "obj"
#         elif rel.startswith("amod"):
#             role = "adj"
#         elif rel.startswith("root") or rel.startswith("cop") or rel.startswith("aux"):
#             role = "verb"
#         roles.append((node["word"], role))
#     return roles

# # ------------------------ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ† Ø§ØµÙ„ÛŒ ------------------------
# def process_sentence(text: str):
#     """Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ + ØªÙˆÚ©Ù†ÛŒØ²ÛŒØ´Ù† + multiword merge + Ù†Ù‚Ø´â€ŒØ¯Ù‡ÛŒ"""
#     norm = normalizer.normalize(text)
#     tokens = word_tokenize(norm)
#     tokens = merge_multiwords(tokens)

#     if dep_parser is not None:
#         return label_roles_with_dep(tokens)
#     else:
#         return label_roles_with_pos(tokens)

# # ------------------------ ØªØ³Øª ------------------------
# if __name__ == "__main__":
#     sent = "Ù…Ø±Ø¯ Ù†Ø±Ù… Ø§ÙØ²Ø§Ø± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ø±Ø¯"
#     result = process_sentence(sent)
#     print("Input:", sent)
#     print("Output:", result)




# # -*- coding: utf-8 -*-
# """
# preprocess_hazm.py  (Normalization & POS tagging using Hazm)
# Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ù…Ø´Ø§Ø¨Ù‡ preprocess.py Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§Ù…Ø§ Ø§Ø² Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Hazm Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:
# - Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†
# - ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù†
# - POS Tagging Ùˆ Dependency parsing
# - Ø­ÙØ¸ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ merge Ú©Ù„Ù…Ø§Øª Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ Ùˆ Ø§ÙØ¹Ø§Ù„ Ù…Ø±Ú©Ø¨
# - Ù†Ù‚Ø´â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù¾ÛŒØ´â€ŒÚ¯Ø±ÙˆÙ‡ (subj, obj, verb, adj)
# """

# from hazm import Normalizer, word_tokenize, POSTagger, DependencyParser
# import re

# # ------------------------ Hazm objects ------------------------
# normalizer = Normalizer()
# pos_tagger = POSTagger(model='resources/pos_tagger.model')  # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Hazm
# # dep_parser = DependencyParser(model='resources/parser.model')  # Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Hazm

# # ------------------------ Ù„ÛŒØ³Øª ØµÙØªâ€ŒÙ‡Ø§ ------------------------
# COMMON_ADJECTIVES = {
#     "Ù…Ø§Ù‡Ø±", "Ø®ÙˆØ´Ù…Ø²Ù‡","Ù…Ù†Ø§Ø³Ø¨", "Ø²ÛŒØ¨Ø§", "Ø¬Ø¯ÛŒØ¯", "Ù‚Ø¯ÛŒÙ…ÛŒ", "Ø¨Ø²Ø±Ú¯", "Ú©ÙˆÚ†Ú©"
# }

# def is_adjective(token: str) -> bool:
#     return token in COMMON_ADJECTIVES

# # ------------------------ Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú†Ù†Ø¯Ø¨Ø®Ø´ÛŒ ------------------------
# DEFAULT_MULTIWORDS = [
#     ["Ù†Ø±Ù…", "Ø§ÙØ²Ø§Ø±"],
#     ["Ù‡ÙˆØ´", "Ù…ØµÙ†ÙˆØ¹ÛŒ"],
#     ["ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", "Ù…Ø§Ø´ÛŒÙ†"],
#     ["Ø§Ø´Ú©Ø§Ù„","Ø²Ø¯Ø§ÛŒÛŒ"]
# ]

# # ------------------------ ØªÙˆØ§Ø¨Ø¹ Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ multiwords Ùˆ compound verbs ------------------------
# def merge_multiwords(tokens, multiwords_list=None):
#     if multiwords_list is None:
#         multiwords_list = DEFAULT_MULTIWORDS
#     merged = []
#     skip = 0
#     i = 0
#     while i < len(tokens):
#         if skip > 0:
#             skip -= 1
#             i += 1
#             continue
#         matched = False
#         for mw in multiwords_list:
#             L = len(mw)
#             if i + L <= len(tokens) and tokens[i:i+L] == mw:
#                 merged.append(" ".join(mw))
#                 skip = L - 1
#                 matched = True
#                 break
#         if not matched:
#             merged.append(tokens[i])
#         i += 1
#     return merged

# def merge_compound_verbs(tokens, light_verbs=None):
#     if light_verbs is None:
#         light_verbs = ['Ú©Ø±Ø¯', 'Ø¯Ø§Ø¯', 'Ú¯Ø±ÙØª', 'Ø²Ø¯', 'Ø´Ø¯', 'Ø§Ø³Øª', 'Ø¨ÙˆØ¯', 'Ø®ÙˆØ§Ù‡Ø¯', 'Ù…ÛŒâ€ŒÚ©Ù†Ø¯', 'Ù…ÛŒâ€ŒØ±ÙˆØ¯']
#     merged_tokens = []
#     skip_next = False
#     for i in range(len(tokens)):
#         if skip_next:
#             skip_next = False
#             continue
#         if i < len(tokens) - 1 and tokens[i+1] in light_verbs:
#             merged_tokens.append(tokens[i] + ' ' + tokens[i+1])
#             skip_next = True
#         else:
#             merged_tokens.append(tokens[i])
#     return merged_tokens

# # ------------------------ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ------------------------
# def normalize_text(text: str) -> str:
#     return normalizer.normalize(text)

# # ------------------------ ØªÙˆÚ©Ù†ÛŒØ²Ù‡ Ú©Ø±Ø¯Ù† ------------------------
# def tokenize_text(text: str):
#     return word_tokenize(text)

# # ------------------------ Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù†Ù‚Ø´â€ŒÙ‡Ø§ Ø¨Ø§ Hazm ------------------------
# def label_roles(tokens, multiwords_list=None, light_verbs=None):
#     """
#     Assign syntactic roles using Hazm POS Tagger + Dependency Parser
#     Returns: list of tuples (token, role)
#     """
#     # 1) merge multiwords & compound verbs
#     merged_tokens = merge_multiwords(tokens, multiwords_list)
#     merged_tokens = merge_compound_verbs(merged_tokens, light_verbs=light_verbs)

#     # 2) Hazm POS tagging
#     tagged = pos_tagger.tag(merged_tokens)  # [(token, POS), ...]

#     # 3) Ù†Ù‚Ø´â€ŒÙ‡Ø§
#     roles = ['other'] * len(merged_tokens)
#     n = len(merged_tokens)
#     if n == 0:
#         return []

#     # ÙØ§Ø¹Ù„ Ùˆ ÙØ¹Ù„ Ø§ÙˆÙ„ÛŒÙ‡
#     roles[0] = 'subj'
#     roles[-1] = 'verb'

#     # 4) Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù…ÙØ¹ÙˆÙ„ Ø¨Ø§ particle "Ø±Ø§"
#     if 'Ø±Ø§' in merged_tokens:
#         ra_idx = merged_tokens.index('Ø±Ø§')
#         # scan left for head of object phrase
#         head_idx = ra_idx - 1
#         while head_idx > 0 and is_adjective(merged_tokens[head_idx]):
#             head_idx -= 1
#         for k in range(head_idx, ra_idx):
#             if k == head_idx:
#                 roles[k] = 'obj'
#             else:
#                 roles[k] = 'adj'
#         # "Ø±Ø§" Ø®ÙˆØ¯Ø´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† other Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯

#     # 5) Ø¨Ø±Ú†Ø³Ø¨â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø³Ø§ÛŒØ± ØµÙØªâ€ŒÙ‡Ø§
#     for i, (tok, pos) in enumerate(tagged):
#         if roles[i] == 'other' and pos in ['ADJ', 'JJ']:
#             roles[i] = 'adj'

#     # 6) Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
#     return [(merged_tokens[i], roles[i]) for i in range(n)]

# # ------------------------ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„ Ø¬Ù…Ù„Ù‡ ------------------------
# def process_sentence(text: str):
#     """Full pipeline: normalize + tokenize + role labeling."""
#     norm = normalize_text(text)
#     tokens = tokenize_text(norm)
#     labeled = label_roles(tokens)
#     return labeled

# # ------------------------ Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª ------------------------
# def read_data(filename, sep=" "):
#     """Read dataset with labels and sentences (binary labels as [t, 1-t])."""
#     labels, sentences = [], []
#     with open(filename, encoding="utf-8") as f:
#         for line in f:
#             line = line.strip()
#             if not line:
#                 continue
#             parts = line.split(sep, 1)
#             if len(parts) < 2:
#                 continue
#             label_str, sentence = parts
#             t = int(label_str)
#             labels.append([t, 1 - t])
#             sentences.append(sentence.strip())
#     return labels, sentences
