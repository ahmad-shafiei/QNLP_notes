\chapter{مفاهیم و مبانی نظری پردازش زبان طبیعی}
\label{chap:theoretical-foundations}

\section{مقدمه}
\label{sec:introduction}

پردازش زبان طبیعی
\ft{Natural Language Processing (NLP)}
 یا NLP یکی از شاخه‌های بنیادین هوش مصنوعی است که به بررسی و پیاده‌سازی تعامل میان کامپیوتر و زبان انسانی می‌پردازد. هدف اصلی این حوزه، توانمندسازی ماشین‌ها برای درک، تفسیر و تولید زبان طبیعی به گونه‌ای است که بتوانند با انسان‌ها به صورت معنادار ارتباط برقرار کنند. این حوزه در دهه‌های اخیر شاهد تحولات شگرفی بوده است، تحولاتی که از روش‌های آماری ساده و مبتنی بر قاعده
 \ft{Rule-driven}
  آغاز شده و به تدریج به سمت مدل‌های یادگیری عمیق پیچیده و در نهایت مدل‌های زبانی بزرگ (LLMs)
\ft{Large Language Models (LLMs)} 
  گسترش یافته است.

در سال‌های اخیر، با پیشرفت‌های چشمگیر در فناوری محاسبات کوانتومی، رویکردهای نوینی برای حل مسائل پردازش زبان طبیعی ظهور کرده‌اند. این رویکردها با بهره‌برداری از ویژگی‌های منحصر به فرد مکانیک کوانتومی از جمله برهم‌نهی، درهم‌تنیدگی و تداخل
\ft{superposition, entanglement and interference}
، امکان مدل‌سازی متفاوت و بالقوه کارآمدتری از ساختار و معنای زبان را فراهم می‌آورند. پردازش زبان طبیعی کوانتومی (QNLP)
\ft{Quantum Natural Language Processing (QNLP)}
  به عنوان یک پارادایم نوین، تلاش می‌کند تا با استفاده از اصول مکانیک کوانتومی، محدودیت‌های روش‌های کلاسیک را برطرف کرده و راه‌حل‌هایی با کارایی بهتر ارائه دهد.

در این فصل، ابتدا به مرور تاریخی تحولات پردازش زبان طبیعی از آغاز تا به امروز خواهیم پرداخت. سپس مدل‌های زبانی بزرگ، تأثیرات و چالش‌های آن‌ها را مورد بررسی قرار می‌دهیم. در ادامه، به معرفی رویکرد کوانتومی در پردازش زبان طبیعی، مزایا و چالش‌های آن پرداخته و رویکردهای مختلف \lr{QNLP} را مقایسه می‌کنیم. به ویژه بر رویکرد
\ft{Distributional Compositional Categorical}
 DisCoCat که مبتنی بر نظریه مقوله‌ها و دستور پیش‌گروهی است تمرکز خواهیم کرد. در نهایت، چالش‌های خاص پردازش زبان فارسی و اهداف این تحقیق را تشریح خواهیم نمود.

\section*{مقدمه‌ای بر پردازش زبان طبیعی و مسیر گذار به پردازش کوانتومی}

پردازش زبان طبیعی
\ft{Natural Language Processing}
 یا \lr{NLP} یکی از شاخه‌های دیرپای هوش مصنوعی است که هدف آن ایجاد امکان درک، تولید و تحلیل زبان انسان توسط ماشین‌هاست. در دهه‌های نخست، روش‌های مبتنی بر قوانین دست‌نویس، منطق صوری و مدل‌های آماری ساده مانند مدل‌های \lr{n-gram} بر این حوزه تسلط داشتند. این روش‌ها اگرچه گامی مهم در فهم ساختار زبان بودند، اما توانایی محدودی در grasp کردن پیچیدگی‌های نحوی و معنایی واقعی زبان طبیعی داشتند.

با ظهور شبکه‌های عصبی عمیق و به‌ویژه مدل‌های مبتنی بر توالی مانند \lr{RNN} و \lr{LSTM}، جهش قابل‌توجهی در عملکرد سامانه‌های زبانی رخ داد. این تحول با معرفی معماری \lr{Transformer} و مکانیسم توجه (\lr{Attention}) وارد مرحله‌ای کاملاً جدید شد که در آن مدل‌های زبانی بزرگ (\lr{LLMs}) توانستند معنا، انسجام و استدلال را در ابعاد بی‌سابقه‌ای یاد بگیرند. با وجود این پیشرفت‌ها، هزینهٔ محاسباتی بسیار بالا، نیاز عظیم به داده و محدودیت‌های بنیادی محاسبات کلاسیک، پژوهشگران را به سوی رویکردهای کاراتر سوق داده است.

در سال‌های اخیر، ایدهٔ بهره‌گیری از محاسبات کوانتومی برای پردازش زبان طبیعی ــ که از آن با عنوان \lr{Quantum NLP (QNLP)} یاد می‌شود ــ مطرح شده است. این حوزه در تلاش است تا از برتری‌های کوانتومی مانند برهم نهی‌، درهم‌تنیدگی و فضاهای هیلبرتی با ابعاد بسیار بالا برای نمایش ساختارهای زبانی بهره گیرد. استفاده از مدارهای کوانتومی، نگاشت‌های تانسوری و مدل‌های ترکیب‌پذیر معنایی می‌تواند امکان پردازش کاراتر و طبیعی‌تر زبان را در آینده فراهم کند؛ به‌ویژه در کاربردهایی مانند دسته‌بندی معنایی، ترجمه، پرسش‌وپاسخ و مدل‌سازی ساختار جمله.

\subsection*{روش \lr{DisCoCat}}
روش \lr{DisCoCat}
\ft{Distributive Compositional Categorical Model}
 یکی از رویکردهای پیشرو برای مدل‌سازی زبان در چارچوب نظریهٔ رده‌ها
 \ft{Category Theory}
  است. در این روش، ترکیب نحوی جمله با استفاده از گرامر پیش گروه
\ft{Pregroup Grammar}  
   توصیف می‌شود و این ساختار نحوی سپس به یک فضا یا مدار کوانتومی نگاشت می‌گردد که در آن معنا از طریق ترکیب تانسوری و نگاشت‌های خطی به‌دست می‌آید. زیبایی \lr{DisCoCat} در این است که ترکیب معنایی زبان را دقیقاً موازی ترکیب‌پذیری فیزیک کوانتومی مدل می‌کند؛ به این ترتیب که نقش‌ها و وابستگی‌های نحوی مستقیماً به ساختارهای خطی در فضاهای هیلبرتی ترجمه می‌شوند. این ویژگی باعث شده که \lr{DisCoCat} پایهٔ نظری بسیاری از پژوهش‌های معاصر \lr{QNLP} باشد و در چارچوب‌هایی مانند \lr{lambeq} به‌صورت عملی و قابل اجرا پیاده‌سازی شود.


حوزهٔ \lr{QNLP} حاصل تلاش مشترک سه پژوهشگر از دانشگاه آکسفورد است: \lr{Mehrnoosh Sadrzadeh}، \lr{Stephen Clark} و \lr{Bob Coecke}.  
صدرزاده بر روی جبر گرامر و ساختارهای ریاضی لازم برای مدل‌سازی نحودار زبان‌های طبیعی کار می‌کرد.  
کلارک به طور تخصصی روی تعبیهٔ واژگان (\lr{Word Embedding}) متمرکز بود؛ روشی که واژه‌ها را به‌صورت عناصر یک فضای برداری نمایش می‌دهد تا ویژگی‌های معنایی آن‌ها به‌طور سازگار حفظ شود.  
در همین زمان، کوک در حال توسعهٔ مکانیک کوانتومی رده‌ای
\ft{Categorical Quantum Mechanics}
 بود؛ چارچوبی که فرآیندهای کوانتومی را به‌صورت ساختارهایی ترکیب‌پذیر و قابل اتصال توصیف می‌کند.  
ترکیب این سه خط پژوهشی زیربنای شکل‌گیری رویکرد نوین \lr{QNLP} و مدل ترکیبی \lr{DisCoCat} شد.


\subsection*{مفاهیم بنیادی در چارچوب \lr{DisCoCat}}

\textbf{گرامر (\lr{Grammar}).}
گرامر مجموعه‌ای از قواعد صوری است که نحوهٔ کنار هم قرار گرفتن واژه‌ها را برای تشکیل جمله مشخص می‌کند. در مدل‌های ترکیبی، گرامر نقش اسکلت ساختاری جمله را ایفا می‌کند و مبنای استخراج روابط نحوی و معنایی است.

\textbf{پارسینگ (\lr{Parsing}).}
پارسینگ فرآیند تحلیل ساختار جمله بر اساس گرامر است؛ یعنی تشخیص نقش هر کلمه و نحوهٔ ترکیب آن‌ها با یکدیگر. برای نمونه در جملهٔ ساده:

\lr{The cat sleeps.}

فعل (\lr{sleep}) به فاعل (\lr{cat}) اعمال می‌شود. در جملهٔ پیچیده‌تر:

\lr{Alice eats an apple.}

پارسینگ تشخیص می‌دهد که \lr{Alice} فاعل، \lr{apple} مفعول و \lr{eats} هستهٔ فعلی است و این سه باید طبق الگوی نحوی استاندارد به هم متصل شوند.

\textbf{پری‌گروپ (\lr{Pregroup}) در برابر گروه (\lr{Group}).}
گروه یک ساختار جبری با عمل دوتایی و وارون‌پذیری کامل است؛ یعنی هر عنصر یک معکوس دارد. اما \lr{Pregroup} ساختاری ضعیف‌تر است که در آن هر نوع زبانی تنها دو شبه‌معکوس (چپ و راست) دارد و ترکیب آن‌ها با حذف‌پذیری کنترل‌شده صورت می‌گیرد. در گرامر نوعی، این ویژگی اجازه می‌دهد که وابستگی‌های نحوی مانند «فاعل–فعل» یا «فعل–مفعول» با عملیات سادهٔ حذف نشان داده شوند.

\textbf{جبر.}
جبر در این زمینه به معنای مجموعه‌ای از ساختارها و قوانین ریاضی برای مدل‌کردن نحوهٔ ترکیب اجزای زبان است. چه در سطح نحوی (مانند \lr{Pregroup}) و چه در سطح معنایی (مانند ضرب تانسوری)، جبر نقش ستون فقرات محاسبات زبان را دارد.

\textbf{معنای توزیعی پ.}
\ft{Distributional Semantics}
این ایده بر این اصل تکیه دارد که «معنای یک واژه را می‌توان از بافت اطراف آن استنباط کرد». واژه‌ها به‌صورت بردارهایی در یک فضای برداری مدل می‌شوند و نزدیکی معنایی از طریق فاصلهٔ این بردارها سنجیده می‌شود. برای مثال بردارهای \lr{king} و \lr{queen} در مدل‌های توزیعی نزدیک‌اند، زیرا در محیط‌های مشابهی به‌کار می‌روند.

\textbf{مدل‌های معناشناسی توزیعی ترکیبی.}
\ft{Compositional Distributional Semantics Models}
این مدل‌ها تلاش می‌کنند دو اصل را با هم ترکیب کنند: (۱) معناشناسی توزیعی برای نمایش برداری واژه‌ها، (۲) ترکیب‌پذیری نحوی برای ساخت معنای کل جمله. آن‌ها می‌کوشند معنای جمله را از طریق ترکیب مناسب بردارهای واژه‌ها و ساختار نحوی حاصل کنند.

\textbf{چارچوب ترکیبی رده‌ای.}
\ft{Categorical Compositional Framework}
این چارچوب از نظریهٔ رده‌ها استفاده می‌کند تا نحوهٔ ترکیب ساختاری (نحو) و ترکیب معنایی را در یک معماری واحد و سازگار توصیف کند. ساختارهای نحوی، نگاشت‌های خطی و ضرب تانسوری همگی به‌صورت فرایندهای ترکیب‌پذیر مدل می‌شوند؛ درست مانند ترکیب سیستم‌ها در مکانیک کوانتومی.

\textbf{معناشناسی توزیعی چندوجهی.}
\ft{Multimodal Distributional Semantics}
این رویکرد معنای واژه‌ها و جملات را تنها از متن استخراج نمی‌کند؛ بلکه از داده‌های چندگانه مانند تصویر، صدا یا تعاملات کاربری نیز بهره می‌گیرد. در این روش، فضاهای برداری چندوجهی ساخته می‌شوند که معنا را از چند منبع هم‌زمان یاد می‌گیرند؛ برای مثال ترکیب تصویر یک «گربه» با توصیف متنی آن برای بهبود معنای برداری واژهٔ \lr{cat}.


%***************************************
%***************************************
%=======================================
\subsection{ظهور یادگیری عمیق و شبکه‌های عصبی پیچیده}
\label{subsec:deep-learning}

با افزایش قدرت محاسباتی سخت‌افزارها، به ویژه واحدهای پردازش گرافیکی \lr{(GPUs)}
\ft{Graphics Processing Units}
، و در دسترس بودن مجموعه داده‌های بزرگ، استفاده از شبکه‌های عصبی عمیق از اواسط دهه 2010 به طور گسترده در پردازش زبان طبیعی رواج یافت. این دوره شاهد پیشرفت‌های چشمگیری در معماری‌های شبکه‌های عصبی بود که توانایی مدل‌سازی پیچیدگی‌های زبان را به طور قابل ملاحظه‌ای افزایش دادند.

یکی از اولین معماری‌های موفق در این دوره، شبکه‌های عصبی بازگشتی
\ft{Recurrent Neural Networks}
 \lr{ (RNN)} بود. این شبکه‌ها با داشتن حلقه‌های بازخورد، قادر بودند اطلاعات از گام‌های قبلی را حفظ کنند و از این رو برای پردازش داده‌های توالی‌ای مانند متن مناسب بودند. با این حال، \lr{RNN} های ساده با مشکل محو شدن گرادیان یا 
 \ft{Vanishing gradient}
  مواجه بودند که مانع از یادگیری وابستگی‌های بلندمدت می‌شد.

برای حل این مشکل، هوخرایتر و اشمیدهوبر در سال 1997 معماری حافظه طولانی کوتاه‌مدت- \lr{ (LSTM)} \cite{hochreiter1997lstm} 
\ft{Long Short-Term Memory}
را معرفی کردند. اگرچه این معماری در اواخر دهه 1990 ابداع شد، اما استفاده گسترده از آن در پردازش زبان طبیعی در دهه 2010 آغاز شد. \lr{LSTM} با استفاده از ساختار پیچیده‌تری شامل دروازه‌های مختلف، توانست اطلاعات بلندمدت را به طور مؤثر حفظ کند. این معماری دارای سه دروازه اصلی است: دروازه فراموشی  که تصمیم می‌گیرد چه اطلاعاتی از حافظه حذف شود، دروازه ورودی که کنترل می‌کند چه اطلاعات جدیدی به حافظه اضافه شود، و دروازه خروجی 
\ft{Forget Gate, Input Gate and Output Gate}
که تعیین می‌کند چه بخشی از حافظه به عنوان خروجی ارائه شود. علاوه بر این، یک حالت سلول
\ft{Cell State}
وجود دارد که به عنوان حافظه بلندمدت عمل می‌کند.

شبکه‌های \lr{LSTM} و نسخه ساده‌تر آن به نام واحدهای بازگشتی دارای دروازه-\lr{GRU} 
\ft{Gated Recurrent Units}
در بسیاری از وظایف پردازش زبان طبیعی از جمله ترجمه ماشینی، تحلیل احساسات و تولید متن به کار گرفته شدند و نتایج قابل توجهی به دست آوردند.

تحول عمده بعدی در سال 2014 با معرفی مکانیزم توجه
\ft{Attention Mechanism}
 توسط بهدانا و همکاران رخ داد. این مکانیزم به مدل اجازه می‌داد تا به جای اینکه تنها بر یک بردار زمینه ثابت تکیه کند، به بخش‌های مختلف ورودی با وزن‌های متفاوت توجه کند. این نوآوری به ویژه در ترجمه ماشینی عصبی تأثیر بسزایی داشت.

اوج این تحولات با انتشار مقاله تأثیرگذار "توجه تنها چیزی است که نیاز دارید" 
\ft{Attention Is All You Need}
 \cite{vaswani2017attention} توسط واسوانی و همکاران در سال 2017 رخ داد. این مقاله معماری ترنسفورمر 
 \ft{Transformer}
 را معرفی کرد که به طور کامل بر مکانیزم توجه استوار است و نیازی به شبکه‌های بازگشتی ندارد. مکانیزم توجه در ترنسفورمر به صورت زیر تعریف می‌شود:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

در این رابطه، $Q$ نمایانگر پرس‌وجوها، $K$ نمایانگر کلیدها و $V$ نمایانگر مقادیر 
\ft{Queries, Keys amd Values}
است. ضریب $\sqrt{d_k}$ برای مقیاس‌بندی استفاده می‌شود تا از مقادیر بسیار بزرگ در تابع \lr{softmax} جلوگیری شود.

معماری ترنسفورمر مزایای قابل توجهی نسبت به معماری‌های قبلی داشت. اول اینکه، امکان پردازش موازی تمام توکن‌های ورودی را فراهم می‌کرد، در حالی که \lr{RNN} ها به دلیل ماهیت توالی‌ای‌شان باید به صورت ترتیبی پردازش شوند. این ویژگی آموزش را بسیار سریع‌تر می‌کرد. دوم، مکانیزم توجه چندسره
\ft{Multi-Head Attention}
  به مدل اجازه می‌داد تا همزمان به جنبه‌های مختلف رابطه میان کلمات توجه کند. سوم اینکه، این معماری توانایی بهتری در مدل‌سازی وابستگی‌های بلندمدت داشت و مقیاس‌پذیری بسیار بالایی را ارائه می‌کرد.
معماری ترنسفورمر به سرعت به یک استاندارد در پردازش زبان طبیعی تبدیل شد و پایه و اساس تمام مدل‌های زبانی بزرگ بعدی را تشکیل داد.

این مقاله که به یکی از پراستنادترین آثار علمی در تاریخ هوش مصنوعی تبدیل شده است، راه‌حلی جسورانه برای محدودیت‌های شبکه‌های بازگشتی ارائه داد. قبل از معرفی ترنسفورمر، \lr{RNN} ها و \lr{LSTM} ها علی‌رغم موفقیت‌هایشان با دو مشکل اساسی مواجه بودند: عدم امکان موازی‌سازی محاسبات به دلیل ماهیت ترتیبی پردازش، و ضعف در یادگیری وابستگی‌های بسیار بلندمدت در جملات طولانی. واسوانی و همکارانش با حذف کامل ساختارهای بازگشتی و تکیه صرف بر مکانیزم توجه، این محدودیت‌ها را برطرف کردند. معماری ترنسفورمر بر پایه دو بخش اصلی کدگذار و رمزگشا ساخته شده که هر کدام از لایه‌های مکانیزم توجه چندسره و شبکه‌های پیشخور تشکیل شده‌اند.

قلب این معماری، مکانیزم توجه مقیاس‌شده ضرب‌نقطه‌ای است که بر اساس سه مفهوم پرس‌وجو، کلید و مقدار عمل می‌کند و به مدل اجازه می‌دهد به طور دینامیک تصمیم بگیرد کدام بخش‌های ورودی برای پردازش هر کلمه اهمیت دارند. استفاده از توجه چندسره به مدل امکان می‌دهد همزمان به جنبه‌های مختلف رابطه میان کلمات از جمله وابستگی‌های نحوی، روابط معنایی و همپوشانی‌های واژگانی توجه کند. برای حفظ اطلاعات مربوط به ترتیب کلمات که با حذف ساختارهای بازگشتی از دست می‌رفت، کدگذاری موقعیت بر اساس توابع سینوسی و کسینوسی به مدل اضافه شد.

نتایج آزمایش‌های این مقاله در ترجمه ماشینی چشمگیر بود، به گونه‌ای که نه تنها رکوردهای قبلی را شکست بلکه زمان آموزش را از هفته‌ها به تنها چند روز کاهش داد. اما تأثیر واقعی این مقاله فراتر از نتایج عددی بود. ترنسفورمر به سرعت پایه تمام مدل‌های بزرگ بعدی شد: \lr{BERT} که تنها از بخش کدگذار استفاده کرد و با پیش‌آموزش دوطرفه رکوردهای متعددی شکست، سری مدل‌های \lr{GPT} که از بخش رمزگشا بهره بردند و در تولید متن طبیعی توانایی شگفت‌انگیز نشان دادند، و در نهایت \lr{GPT-3} با ۱۷۵ میلیارد پارامتر که نقطه عطفی در تاریخ هوش مصنوعی شد. امروزه تقریباً تمام سیستم‌های پیشرفته پردازش زبان از مترجم‌های خودکار تا دستیارهای هوشمند مانند \lr{ChatGPT} بر پایه این معماری استوارند، و حتی فراتر از پردازش زبان، در بینایی کامپیوتر، زیست‌شناسی محاسباتی و پیش‌بینی ساختار پروتئین‌ها نیز کاربرد یافته است. در [پیوست 
\ref{sec:attention-paper-review}]
، تحلیل جامع‌تری از این مقاله تأثیرگذار و نقش آن در تحول هوش مصنوعی زبانی ارائه شده است.

\subsection{مدل‌های زبانی بزرگ و تحولات اخیر}
\label{subsec:llms}

پس از معرفی معماری ترنسفورمر، تحقیقات به سمت ساخت مدل‌های بزرگ‌تر و قدرتمندتر پیش رفت. ایده اصلی این بود که با افزایش اندازه مدل و حجم داده‌های آموزشی، عملکرد مدل در طیف وسیعی از وظایف بهبود می‌یابد. این رویکرد به ظهور مدل‌های زبانی بزرگ-\lr{ LLMs} 
\ft{Large Language Models}
منجر شد که توانایی‌های شگفت‌انگیزی در فهم و تولید زبان طبیعی از خود نشان دادند.

یکی از اولین و تأثیرگذارترین مدل‌های این دوره، \lr{BERT}  
\ft{Bidirectional Encoder Representations from Transformers}
\cite{devlin2019bert}
بود که در اکتبر 2018 توسط محققان گوگل معرفی شد. نوآوری اصلی \lr{BERT} در استفاده از پیش‌آموزش دوطرفه بود، به این معنا که مدل هنگام پیش‌بینی یک کلمه، هم به کلمات قبل و هم به کلمات بعد از آن توجه می‌کند. این رویکرد برخلاف مدل‌های زبانی سنتی بود که تنها به صورت یک‌طرفه (از چپ به راست یا بالعکس) عمل می‌کردند.

\lr{BERT}
 از دو وظیفه پیش‌آموزشی استفاده می‌کند: اول، مدل‌سازی زبانی پوشیده 
\ft{Masked Language Modeling}
 \lr{ (MLM)}
  که در آن برخی کلمات به صورت تصادفی پوشیده شده و مدل باید آن‌ها را پیش‌بینی کند. دوم، پیش‌بینی جمله بعدی- \lr{ NSP}
 \ft{Next Sentence Prediction} 
   که مدل باید تشخیص دهد آیا دو جمله در متن اصلی پشت سر هم آمده‌اند یا خیر. پس از پیش‌آموزش بر روی پیکره‌های عظیم متنی، \lr{BERT} می‌تواند با تنظیم ظریف
 \ft{Fine-tuning}  
     بر روی مجموعه داده‌های کوچک‌تر برای وظایف خاص مانند پاسخ به پرسش، تحلیل احساسات و شناسایی موجودیت‌های اسمی آموزش ببیند. این رویکرد منجر به شکستن رکوردهای متعددی در معیارهای سنجش استاندارد
     \ft{benchmarks}
     پردازش زبان طبیعی شد.

در مسیر موازی، مدل‌های تولیدی 
\ft{Generative models}
 نیز در حال پیشرفت بودند. سری مدل‌های \lr{GPT}
 \ft{Generative Pre-trained Transformer}
   توسط شرکت \lr{OpenAI} توسعه یافت. \lr{GPT-1} در سال 2018 معرفی شد و نشان داد که یک مدل زبانی بزرگ که بر روی پیکره گسترده‌ای پیش‌آموزش دیده، می‌تواند با حداقل تنظیم در وظایف مختلف عملکرد خوبی داشته باشد.

\lr{GPT-2} در سال 2019 با 1.5 میلیارد پارامتر معرفی شد و توانایی تولید متن‌های منسجم و طبیعی را نشان داد. در سال 2020، \lr{GPT-3} \cite{brown2020gpt3}
 با 175 میلیارد پارامتر، نقطه عطفی در تاریخ پردازش زبان طبیعی بود. این مدل توانایی یادگیری چندوظیفه‌ای
\ft{Few-shot learning}
 \lr{} را به نمایش گذاشت، به این معنا که با دیدن تنها چند مثال از یک وظیفه جدید، می‌توانست آن وظیفه را انجام دهد بدون اینکه نیازی به تنظیم ظریف پارامترهایش باشد. حتی در حالت یادگیری بدون مثال
 \ft{zero-shot learning}
، \lr{GPT-3}
 در بسیاری از وظایف عملکرد قابل قبولی داشت.

در سال‌های اخیر، مدل‌های پیشرفته‌تری نیز معرفی شده‌اند. \lr{GPT-4} که در سال 2023 عرضه شد، توانایی‌های چندوجهی 
\ft{multimodal}
 دارد و می‌تواند علاوه بر متن، با تصاویر نیز کار کند. مدل‌های دیگری مانند \lr{PaLM} از گوگل، \lr{LLaMA} از متا، و \lr{Claude} از \lr{Anthropic} نیز در این حوزه فعالیت می‌کنند. این مدل‌ها در کاربردهای متنوعی از ترجمه ماشینی گرفته تا کدنویسی خودکار، تحلیل داده، و حتی تحقیقات علمی مورد استفاده قرار می‌گیرند.

مدل‌های زبانی بزرگ تأثیرات عمیق و گسترده‌ای بر جامعه، صنعت و تحقیقات علمی داشته‌اند. بهبود چشمگیر در کیفیت ترجمه ماشینی، یکی از بارزترین دستاوردهای این مدل‌ها است. سیستم‌های ترجمه مبتنی بر \lr{LLMs} توانسته‌اند کیفیت ترجمه را به سطحی برسانند که در بسیاری موارد با ترجمه انسانی قابل مقایسه است. دستیارهای هوشمند و چت‌بات‌های پیشرفته مانند \lr{ChatGPT}، \lr{Gamini} و \lr{Claude} توانسته‌اند تعاملات طبیعی و معناداری با کاربران برقرار کنند و در حوزه‌های مختلف از پشتیبانی مشتری تا آموزش و یادگیری کمک کنند.

خلاصه‌سازی خودکار اسناد، تولید محتوای متنی، نوشتن کد برنامه‌نویسی، و کمک به تحقیقات علمی و پزشکی از دیگر کاربردهای مهم این مدل‌ها هستند. در حوزه تحقیقات، \lr{LLMs} می‌توانند در بررسی ادبیات، تحلیل داده‌ها، و حتی پیشنهاد فرضیه‌های جدید به محققان کمک کنند.

با این حال، مدل‌های زبانی بزرگ با چالش‌ها و مسائل جدی‌ای نیز مواجه هستند که نیازمند توجه و تحقیقات بیشتر است. یکی از اساسی‌ترین چالش‌ها، نیاز به منابع محاسباتی عظیم برای آموزش و اجرای این مدل‌هاست. به عنوان مثال، آموزش \lr{GPT-3} با 175 میلیارد پارامتر نیازمند هزاران ساعت محاسبه بر روی صدها پردازنده گرافیکی قدرتمند بود که هزینه‌ای در حدود میلیون‌ها دلار دارد. این امر دسترسی به این فناوری را محدود به شرکت‌ها و سازمان‌های بزرگ می‌کند و شکاف دیجیتال را تشدید می‌کند.

مصرف انرژی و اثرات زیست‌محیطی این مدل‌ها نیز نگرانی جدی است. آموزش یک مدل زبانی بزرگ می‌تواند ردپای کربنی قابل توجهی داشته باشد که معادل سوخت مصرفی چندین خودرو در طول عمرشان است. این موضوع اهمیت توسعه الگوریتم‌های کارآمدتر و استفاده از انرژی‌های تجدیدپذیر را برجسته می‌کند.

تعصب و انصاف
\ft{bias and fairness}
 یکی دیگر از چالش‌های اساسی است. این مدل‌ها بر روی داده‌های گسترده‌ای از اینترنت آموزش می‌بینند که ممکن است حاوی تعصبات اجتماعی، نژادی، جنسیتی و فرهنگی باشند. در نتیجه، مدل نیز این تعصبات را یاد می‌گیرد و در خروجی‌هایش بازتاب می‌دهد. این موضوع می‌تواند منجر به تصمیم‌گیری‌های ناعادلانه در کاربردهای حساس مانند استخدام، اعتباردهی و قضاوت‌های قانونی شود.

قابلیت تفسیر 
\ft{Interpretability}
محدود این مدل‌ها نیز چالش مهمی است. مدل‌های زبانی بزرگ به صورت "جعبه سیاه" عمل می‌کنند، به این معنا که حتی طراحان آن‌ها نمی‌توانند به طور دقیق توضیح دهند که چرا مدل یک پاسخ خاص را تولید کرده است. این عدم شفافیت در کاربردهای بحرانی مانند تشخیص پزشکی یا تصمیمات قانونی می‌تواند مشکل‌ساز باشد.

یکی از مشکلات شناخته شده \lr{LLMs}، پدیده توهم یا هذیان گویی
\ft{Hallucination}
است. این مدل‌ها گاهی اطلاعات نادرست یا جعلی را با اطمینان و قاطعیت بیان می‌کنند، به گونه‌ای که تشخیص درستی یا نادرستی آن‌ها برای کاربر عادی دشوار است. این موضوع می‌تواند منجر به انتشار اطلاعات غلط و گمراه‌کنندگی شود.

علاوه بر این، مسائل امنیتی، حریم خصوصی، حقوق مالکیت فکری، و سوءاستفاده احتمالی از این فناوری برای تولید محتوای مخرب یا گمراه‌کننده نیز از دیگر چالش‌های مهم هستند که نیازمند راهکارهای فنی، قانونی و اخلاقی هستند.

\section{رویکرد کوانتومی در پردازش زبان طبیعی}
\label{sec:quantum-nlp}

با وجود پیشرفت‌های چشمگیر در مدل‌های زبانی بزرگ کلاسیک، محدودیت‌ها و چالش‌های ذکر شده در بخش قبل انگیزه‌ای برای جستجوی رویکردهای جایگزین یا مکمل شده است. یکی از رویکردهای نوظهور که در سال‌های اخیر توجه محققان را به خود جلب کرده، استفاده از محاسبات کوانتومی در پردازش زبان طبیعی است. این رویکرد با بهره‌گیری از اصول بنیادین مکانیک کوانتومی، چشم‌اندازهای جدیدی را برای مدل‌سازی زبان و معنا فراهم می‌آورد.

\subsection{مبانی محاسبات کوانتومی}
\label{subsec:quantum-basics}

برای درک رویکرد کوانتومی در پردازش زبان طبیعی، ابتدا لازم است با مفاهیم پایه‌ای محاسبات کوانتومی آشنا شویم. محاسبات کوانتومی بر اساس اصول مکانیک کوانتومی عمل می‌کند و از ویژگی‌هایی بهره می‌برد که در محاسبات کلاسیک وجود ندارند.

واحد اطلاعات کلاسیک، بیت است که می‌تواند دو مقدار 0 یا 1 داشته باشد. در محاسبات کوانتومی، واحد اساسی اطلاعات، کیوبیت نامیده می‌شود. یک کیوبیت می‌تواند در حالت برهم‌نهی قرار گیرد، به این معنا که به صورت همزمان در ترکیبی از هر دو حالت $|0\rangle$ و $|1\rangle$ باشد. این خاصیت به صورت زیر نمایش داده می‌شود:
\begin{equation}
|\psi\rangle = \alpha|0\rangle + \beta|1\rangle
\end{equation}
در این رابطه، $\alpha$ و $\beta$ اعداد مختلط هستند که در آن‌ها $|\alpha|^2$ احتمال مشاهده حالت $|0\rangle$ و $|\beta|^2$ احتمال مشاهده حالت $|1\rangle$ را نشان می‌دهد. قید نرمال‌سازی $|\alpha|^2 + |\beta|^2 = 1$ باید برقرار باشد. این ویژگی به یک کیوبیت اجازه می‌دهد تا اطلاعات بیشتری نسبت به یک بیت کلاسیک نمایش دهد.

یکی دیگر از ویژگی‌های بنیادین مکانیک کوانتومی، درهم‌تنیدگی است. وقتی دو یا چند کیوبیت درهم تنیده می‌شوند، حالت یک کیوبیت به طور جدایی‌ناپذیری به حالت کیوبیت‌های دیگر وابسته می‌شود، به گونه‌ای که نمی‌توان آن‌ها را به صورت مستقل توصیف کرد. یک مثال کلاسیک از حالت درهم‌تنیده، حالت بِل است:
\begin{equation}
|\Phi^+\rangle = \frac{1}{\sqrt{2}}(|00\rangle + |11\rangle)
\end{equation}
در این حالت، اگر کیوبیت اول را اندازه‌گیری کنیم و نتیجه 0 باشد، کیوبیت دوم نیز قطعاً 0 خواهد بود، و اگر کیوبیت اول 1 باشد، کیوبیت دوم نیز 1 خواهد بود. این همبستگی قوی، بدون اینکه هیچ اطلاعاتی میان کیوبیت‌ها منتقل شود، برقرار است و یکی از عجیب‌ترین پدیده‌های مکانیک کوانتومی محسوب می‌شود.

ویژگی سوم که در محاسبات کوانتومی نقش مهمی دارد، تداخل
\ft{Interference}
 است. در مکانیک کوانتومی، دامنه‌های احتمال می‌توانند با یکدیگر تداخل کنند. تداخل سازنده باعث افزایش احتمال یک پیامد خاص می‌شود، در حالی که تداخل مخرب احتمال را کاهش می‌دهد. الگوریتم‌های کوانتومی از این ویژگی برای تقویت احتمال پاسخ‌های صحیح و کاهش احتمال پاسخ‌های نادرست استفاده می‌کنند.

این سه ویژگی - برهم‌نهی، درهم‌تنیدگی و تداخل - پایه و اساس قدرت محاسباتی کامپیوترهای کوانتومی را تشکیل می‌دهند و امکانات جدیدی برای حل مسائل پیچیده فراهم می‌آورند.

\subsection{یادگیری ماشین کوانتومی}
\label{subsec:qml}

یادگیری ماشین کوانتومی
 \lr{ (QML)}
 \ft{Quantum Machine Learning} 
  \cite{biamonte2017quantum,schuld2015quantum}
   به کاربرد الگوریتم‌ها و مدارات کوانتومی در حل مسائل یادگیری ماشین می‌پردازد. این حوزه در تلاقی دو رشته یادگیری ماشین و محاسبات کوانتومی قرار دارد و هدف آن بهره‌گیری از مزایای هر دو است.

در عصر فعلی که \lr{ (NISQ)}
\ft{Noisy Intermediate-Scale Quantum}
 نامیده می‌شود، کامپیوترهای کوانتومی با تعداد محدود کیوبیت (معمولاً کمتر از چند صد) و با نویز قابل توجه در دسترس هستند. برای استفاده مؤثر از این سخت‌افزارهای محدود، الگوریتم‌های وردشی کوانتومی
 \ft{Variational Quantum Algorithms }
 \lr{(VQA)}
  توسعه یافته‌اند. این الگوریتم‌ها ترکیبی از پردازش کوانتومی و کلاسیک هستند و از مدارات کوانتومی پارامتری \lr{(PQC)}
  \ft{\lr{Parameterized Quantum Circuits }}
   استفاده می‌کنند.

ساختار کلی یک الگوریتم تغییراتی کوانتومی به صورت زیر است: ابتدا داده‌های ورودی کلاسیک به یک حالت کوانتومی تبدیل می‌شوند، این فرآیند را کدگذاری
\ft{Encoding}
می‌نامند. سپس یک مدار کوانتومی پارامتری بر روی این حالت اعمال می‌شود که شامل دروازه‌های کوانتومی با پارامترهای قابل تنظیم است. این پارامترها معمولاً زوایای چرخش دروازه‌های کوانتومی هستند. در نهایت، حالت کوانتومی نهایی اندازه‌گیری شده و به خروجی کلاسیک تبدیل می‌شود. یک بهینه‌ساز کلاسیک، پارامترهای مدار را به منظور کمینه کردن یک تابع هزینه تنظیم می‌کند.

نگاشت ویژگی‌های کوانتومی
\ft{Quantum Feature Maps}
نقش مهمی در یادگیری ماشین کوانتومی دارد. این نگاشت‌ها داده‌های کلاسیک را به فضای هیلبرت  کوانتومی با ابعاد بسیار بالا منتقل می‌کنند. در این فضای با ابعاد بالاتر، الگوهایی که در فضای کلاسیک خطی قابل تفکیک نیستند ممکن است قابل تفکیک شوند. این ایده مشابه استفاده از هسته یا \lr{kernel} در ماشین‌های بردار پشتیبان \lr{ (SVM)}
\ft{Support Vector Machines}
است.

مفهوم \lr{Ansatz} در محاسبات کوانتومی اهمیت ویژه‌ای دارد. \lr{Ansatz} به معنای یک حدس آموزش‌یافته یا ساختار اولیه است که برای حل یک مسئله انتخاب می‌شود. در زمینه یادگیری ماشین کوانتومی، \lr{Ansatz} به ساختار و آرایش دروازه‌های کوانتومی در مدار پارامتری اشاره دارد. انتخاب \lr{Ansatz} مناسب تأثیر مستقیم بر عملکرد مدل دارد. برخی از \lr{Ansatz} های معروف عبارتند از: \lr{Hardware Efficient Ansatz} که با توجه به محدودیت‌های سخت‌افزار طراحی می‌شود،
 \lr{IQP Ansatz} 
 \ft{Instantaneous Quantum Polynomial Ansatz}
 که در کاربردهای \lr{QNLP} کاربرد دارد، و \lr{UCCSD Ansatz} که در شیمی کوانتومی استفاده می‌شود.

در زمینه پردازش زبان طبیعی، یادگیری ماشین کوانتومی می‌تواند به روش‌های مختلفی به کار گرفته شود. یکی از رویکردها، استفاده از مدارات کوانتومی برای یادگیری نمایش‌های برداری کلمات است. روش دیگر، ساخت طبقه‌بند‌های کوانتومی برای وظایف مانند تحلیل احساسات یا دسته‌بندی متن است که در آن معنای کلی یک جمله به یک حالت کوانتومی نگاشت می‌شود و سپس اندازه‌گیری برای تعیین برچسب انجام می‌گیرد.

\subsection{مزایای رویکرد کوانتومی در پردازش زبان طبیعی}
\label{subsec:quantum-advantages}

رویکرد کوانتومی در پردازش زبان طبیعی مزایای ذاتی و منحصر به فردی دارد که از ویژگی‌های بنیادین مکانیک کوانتومی ناشی می‌شود.

یکی از مهم‌ترین مزایا، نمایش طبیعی ترکیب‌پذیری
\ft{compositionality} 
زبان است. زبان طبیعی ذاتاً ترکیبی است، به این معنا که معنای یک جمله از معنای اجزای آن و نحوه ترکیب آن‌ها حاصل می‌شود. در مکانیک کوانتومی، ترکیب سیستم‌ها از طریق ضرب تانسوری یا ضرب تانسوری
\ft{tensor product} 
در فضای هیلبرت انجام می‌شود. این تناظر طبیعی میان ساختار ترکیبی زبان و ساختار ریاضی فضای هیلبرت، امکان مدل‌سازی مستقیم و شهودی از معناشناسی زبان را فراهم می‌آورد. به عنوان مثال، معنای عبارت "سیب قرمز" می‌تواند به صورت ترکیب تانسوری حالت‌های کوانتومی مربوط به "سیب" و "قرمز" نمایش داده شود.

مزیت دیگر، توانایی مدل‌سازی ابهام و چندمعنایی است. در زبان طبیعی، بسیاری از کلمات و جملات دارای معانی متعدد هستند که بسته به زمینه تعیین می‌شوند. برهم‌نهی کوانتومی به طور طبیعی می‌تواند این ابهامات معنایی را نمایش دهد. یک کلمه چندمعنا می‌تواند به عنوان یک برهم‌نهی از حالت‌های مختلف معنایی مدل شود، و فرآیند درک زمینه می‌تواند به عنوان فروپاشی
\ft{collapse} 
این برهم‌نهی به یک معنای خاص در نظر گرفته شود.

کاهش ابعاد مورد نیاز نیز از مزایای قابل توجه است. فضای هیلبرت کوانتومی ابعاد نمایی نسبت به تعداد کیوبیت دارد. به عنوان مثال، $n$ کیوبیت می‌تواند فضایی با $2^n$ بعد را نمایش دهد. این ویژگی امکان نمایش معانی پیچیده و روابط معنایی را با تعداد نسبتاً کمی از کیوبیت فراهم می‌آورد، در حالی که در روش‌های کلاسیک ممکن است به بردارهای با ابعاد بسیار بالا نیاز باشد.

درهم‌تنیدگی کوانتومی قابلیت منحصر به فردی برای مدل‌سازی همبستگی‌های غیرکلاسیک فراهم می‌کند. در زبان طبیعی، کلمات و مفاهیم اغلب به گونه‌ای با یکدیگر مرتبط هستند که نمی‌توان آن‌ها را به صورت مستقل توصیف کرد. برای مثال، در عبارت "شاه و ملکه"، این دو مفهوم به شدت به یکدیگر وابسته‌اند و درهم‌تنیدگی می‌تواند این وابستگی معنایی پیچیده را به طور طبیعی نمایش دهد.

علاوه بر این، رویکرد کوانتومی پتانسیل کارایی محاسباتی در برخی وظایف را دارد. اگرچه این موضوع هنوز در مراحل اولیه تحقیق است، اما امید می‌رود که در آینده الگوریتم‌های کوانتومی بتوانند برخی محاسبات مربوط به پردازش زبان را سریع‌تر از روش‌های کلاسیک انجام دهند، به ویژه در مواردی که نیاز به جستجو در فضای حالت بزرگ است.

\subsection{چالش‌های رویکرد کوانتومی}
\label{subsec:quantum-challenges}

با وجود مزایای مهم رویکرد کوانتومی، این حوزه با چالش‌های جدی و بنیادینی مواجه است که بر توسعه و کاربرد عملی آن تأثیر می‌گذارد.

یکی از اساسی‌ترین چالش‌ها، محدودیت‌های سخت‌افزاری فعلی است. در حال حاضر ما در عصر کوانتومی میان‌مقیاس پرنویز یا عصر کوانتومی میان‌مقیاس پرنویز
\ft{NISQ Era (Noisy Intermediate-Scale Quantum)} 
قرار داریم. کامپیوترهای کوانتومی موجود دارای تعداد محدودی کیوبیت هستند (معمولاً بین 50 تا چند صد کیوبیت) و این کیوبیت‌ها دچار نویز و خطای قابل توجهی هستند. این خطاها از منابع مختلفی مانند نقص در دروازه‌های کوانتومی، خطای اندازه‌گیری، و تعامل ناخواسته با محیط ناشی می‌شوند. در نتیجه، نتایج محاسبات کوانتومی همیشه دقیق نیستند و نیاز به تکرار آزمایش‌ها و متوسط‌گیری نتایج وجود دارد.

واهلیدگی
\ft{decoherence} 
یکی دیگر از چالش‌های اساسی است. سیستم‌های کوانتومی بسیار حساس به تعاملات با محیط اطرافشان هستند. هر تعامل ناخواسته با محیط می‌تواند باعث از دست رفتن خواص کوانتومی مانند برهم‌نهی و درهم‌تنیدگی شود. این فرآیند که واهلیدگی نامیده می‌شود، معمولاً در مقیاس‌های زمانی بسیار کوتاه (میکروثانیه تا میلی‌ثانیه) رخ می‌دهد. بنابراین، محاسبات کوانتومی باید به سرعت انجام شوند و عمق مدار
\ft{circuit depth} 
(تعداد لایه‌های دروازه‌های کوانتومی) باید محدود باشد. این محدودیت، پیچیدگی مدارات قابل اجرا را کاهش می‌دهد.

محدودیت تعداد کیوبیت نیز چالش عملی مهمی است. بسیاری از مسائل واقعی پردازش زبان طبیعی نیازمند پردازش جملات طولانی یا اسناد بزرگ هستند. نگاشت این مسائل به مدارات کوانتومی ممکن است به تعداد زیادی کیوبیت نیاز داشته باشد که فراتر از ظرفیت سخت‌افزارهای فعلی است. این محدودیت باعث می‌شود که تحقیقات فعلی عمدتاً بر روی مسائل کوچک و نمونه‌های اثبات مفهوم یا اثبات مفهوم
\ft{proof-of-concept} 
متمرکز شوند.

مقیاس‌پذیری
\ft{scalability} 
چالش دیگری است. حتی اگر بتوانیم یک وظیفه ساده پردازش زبان طبیعی را بر روی کامپیوتر کوانتومی انجام دهیم، گسترش آن به مسائل واقعی و پیچیده دشوار است. بسیاری از کاربردهای واقعی نیازمند پردازش میلیون‌ها کلمه، پیکره‌های بزرگ متنی، و مدل‌های با میلیاردها پارامتر هستند. نحوه مقیاس‌دهی رویکردهای کوانتومی به این ابعاد، هنوز یک سؤال باز است.

ارتباط میان دنیای کوانتومی و کلاسیک نیز چالش‌برانگیز است. داده‌های ورودی (متن) و خروجی (مثلاً برچسب دسته‌بندی) معمولاً کلاسیک هستند و باید به حالت‌های کوانتومی تبدیل و سپس دوباره به اطلاعات کلاسیک برگردانده شوند. این فرآیند کدگذاری و رمزگشایی می‌تواند هزینه‌بر باشد. علاوه بر این، اندازه‌گیری کوانتومی ذاتاً احتمالی است و اطلاعات کامل حالت کوانتومی را از بین می‌برد، که ممکن است منجر به از دست رفتن بخشی از اطلاعات معنایی شود.

در نهایت، کمبود الگوریتم‌ها و روش‌های بهینه برای کاربردهای پردازش زبان طبیعی کوانتومی نیز یک چالش است. بسیاری از روش‌های موجود هنوز در مراحل اولیه تحقیق هستند و نیاز به توسعه بیشتر دارند. همچنین، مقایسه عادلانه میان روش‌های کوانتومی و کلاسیک دشوار است، زیرا سخت‌افزار کوانتومی هنوز به بلوغ نرسیده و نمی‌توان قضاوت قطعی در مورد برتری یا عدم برتری آن در مقایسه با روش‌های کلاسیک بهینه‌شده کرد.

\subsection{وضعیت فعلی و چشم‌انداز آینده}
\label{subsec:quantum-future}

در حال حاضر، پردازش زبان طبیعی کوانتومی در مرحله تحقیقات بنیادین و آزمایش‌های اثبات مفهوم قرار دارد. محققان در دانشگاه‌ها و شرکت‌های فناوری در حال کاوش در پتانسیل‌ها و محدودیت‌های این رویکرد هستند. پلتفرم‌های ابری کوانتومی مانند \lr{IBM Quantum Experience}، \lr{Google Quantum AI}، \lr{Amazon Braket}، و \lr{IonQ} امکان دسترسی به کامپیوترهای کوانتومی واقعی را برای محققان فراهم کرده‌اند، که این امر باعث تسریع تحقیقات در این حوزه شده است.

اکثر کارهای تحقیقاتی فعلی بر روی وظایف ساده پردازش زبان طبیعی مانند دسته‌بندی جملات کوتاه، تحلیل احساسات پایه، و سایر مسائل با مجموعه داده‌های کوچک متمرکز است. این محدودیت عمدتاً به دلیل قید و بندهای سخت‌افزاری کنونی است. با این حال، نتایج اولیه نشان‌دهنده امکان‌پذیری رویکرد و پتانسیل‌های امیدوارکننده آن هستند.

چشم‌انداز آینده پردازش زبان طبیعی کوانتومی به عوامل متعددی بستگی دارد. یکی از مهم‌ترین آن‌ها، پیشرفت در سخت‌افزار کوانتومی است. انتظار می‌رود که در دهه‌های آینده، کامپیوترهای کوانتومی با تعداد کیوبیت بیشتر، نرخ خطای پایین‌تر، و زمان‌های انسجام طولانی‌تر در دسترس قرار گیرند. توسعه کدهای تصحیح خطای کوانتومی
\ft{Quantum Error Correction (QEC)} 
نیز می‌تواند به ساخت کامپیوترهای کوانتومی قابل اعتمادتر کمک کند، که این امر مسیر را برای اجرای الگوریتم‌های پیچیده‌تر هموار می‌کند.

از سوی دیگر، توسعه الگوریتم‌های الهام‌گرفته از کوانتوم
\ft{quantum-inspired algorithms} 
برای سیستم‌های کلاسیک نیز مسیر امیدوارکننده‌ای است. این الگوریتم‌ها از ایده‌های کوانتومی مانند ضرب تانسوری و ساختارهای ترکیبی استفاده می‌کنند اما بر روی کامپیوترهای کلاسیک اجرا می‌شوند. ممکن است این رویکردها بتوانند بدون نیاز به سخت‌افزار کوانتومی، برخی مزایای رویکرد کوانتومی را به ارمغان بیاورند.

رویکرد ترکیبی کوانتومی-کلاسیک
\ft{Hybrid Quantum-Classical} 
احتمالاً واقعی‌ترین مسیر برای کاربردهای نزدیک آینده است. در این رویکرد، بخش‌هایی از محاسبات که از مزایای کوانتومی بهره‌مند می‌شوند بر روی پردازنده کوانتومی اجرا می‌شوند، در حالی که بقیه محاسبات توسط رایانه‌های کلاسیک کارآمد انجام می‌شود. این تقسیم کار می‌تواند به استفاده بهینه از منابع محدود کوانتومی منجر شود.

همچنین انتظار می‌رود که کاربردهای تخصصی در حوزه‌های خاص پردازش زبان طبیعی توسعه یابند. به جای تلاش برای جایگزینی کامل مدل‌های کلاسیک، رویکرد کوانتومی ممکن است در وظایف خاصی که با ویژگی‌های کوانتومی سازگاری بیشتری دارند، مزیت رقابتی پیدا کند. برای مثال، وظایفی که نیازمند مدل‌سازی روابط پیچیده معنایی یا ساختارهای بسیار ترکیبی هستند، ممکن است کاندیدهای مناسبی برای رویکرد کوانتومی باشند.

\section{رویکردهای مختلف در پردازش زبان طبیعی کوانتومی}
\label{sec:qnlp-approaches}

محققان رویکردهای متنوعی را برای ترکیب محاسبات کوانتومی و پردازش زبان طبیعی پیشنهاد کرده‌اند. این رویکردها از نظر میزان اتکا به اصول کوانتومی، نحوه مدل‌سازی ساختار زبان، و استراتژی‌های بهینه‌سازی با یکدیگر متفاوت هستند. در این بخش، برخی از مهم‌ترین این رویکردها را بررسی می‌کنیم.

\subsection{LSTM کوانتومی و شبکه‌های عصبی بازگشتی کوانتومی}
\label{subsec:quantum-lstm}

یکی از رویکردهای شهودی برای ورود محاسبات کوانتومی به پردازش زبان طبیعی، کوانتومی کردن معماری‌های موجود و مؤثر مانند حافظه طولانی کوتاه‌مدت است. ایده اصلی این است که لایه‌های کلاسیک شبکه عصبی را با مدارات کوانتومی پارامتری جایگزین کنیم. در این رویکرد، وضعیت پنهان
\ft{hidden state} 
و حافظه سلول
\ft{cell state} 
به حالت‌های کوانتومی نگاشت می‌شوند، و دروازه‌های مختلف حافظه طولانی کوتاه‌مدت (دروازه فراموشی، ورودی، خروجی) با استفاده از دروازه‌های کوانتومی پیاده‌سازی می‌شوند.

در یک حافظه طولانی کوتاه‌مدت کوانتومی، هر گام زمانی شامل فرآیندهای زیر است: ابتدا ورودی کلاسیک (مثلاً بردار نمایش یک کلمه) به یک حالت کوانتومی کدگذاری می‌شود. سپس این حالت با حالت پنهان قبلی (که خود یک حالت کوانتومی است) ترکیب می‌شود. یک مدار کوانتومی پارامتری که شامل دروازه‌های چرخشی و درهم‌تنیدگی است، بر روی این حالت ترکیبی اعمال می‌شود تا حالت جدید را تولید کند. در نهایت، بخشی از این حالت اندازه‌گیری و به خروجی کلاسیک تبدیل می‌شود.

این رویکرد مزایایی دارد. از جمله اینکه می‌تواند از قدرت نمایشی فضای هیلبرت کوانتومی برای مدل‌سازی وابستگی‌های پیچیده زمانی بهره ببرد. همچنین درهم‌تنیدگی کوانتومی می‌تواند روابط غیرخطی میان اجزای مختلف را به طور طبیعی نمایش دهد. با این حال، چالش‌های قابل توجهی نیز وجود دارد. نیاز به تبدیل مکرر میان فضای کلاسیک و کوانتومی در هر گام زمانی، هزینه محاسباتی قابل توجهی دارد و ممکن است منجر به از دست رفتن اطلاعات شود. علاوه بر این، برای دنباله‌های طولانی، نیاز به حفظ انسجام کوانتومی در طول چندین گام زمانی دشوار است. پیچیدگی مدار کوانتومی نیز به سرعت افزایش می‌یابد که با محدودیت‌های سخت‌افزاری فعلی سازگار نیست.

\subsection{مکانیزم‌های توجه کوانتومی}
\label{subsec:quantum-attention}

رویکرد دیگر، پیاده‌سازی مکانیزم توجه
\ft{attention mechanism} 
با استفاده از الگوریتم‌های کوانتومی است. مکانیزم توجه که در معماری‌های مدرن مانند ترنسفورمر نقش کلیدی دارد، اساساً محاسبه میزان شباهت یا ارتباط میان بخش‌های مختلف ورودی است. در نسخه کوانتومی، می‌توان از آزمون تعویض کوانتومی
\ft{quantum swap test} 
برای محاسبه شباهت میان دو حالت کوانتومی استفاده کرد. همچنین تقویت دامنه یا تقویت دامنه
\ft{amplitude amplification} 
می‌تواند برای تأکید بر بخش‌های مهم‌تر ورودی به کار رود. این رویکرد پتانسیل کاهش پیچیدگی محاسباتی را دارد، اما پیاده‌سازی عملی آن در سخت‌افزارهای فعلی چالش‌برانگیز است.

\subsection{مدل‌های نمایش کوانتومی برای کلمات}
\label{subsec:quantum-embedding}

در این رویکرد، به جای استفاده از بردارهای کلاسیک برای نمایش کلمات (مانند \lr{Word2Vec})، هر کلمه به یک حالت کوانتومی نگاشت می‌شود. این نمایش‌های کوانتومی یا تعبیه‌های کوانتومی
\ft{quantum embeddings} 
می‌توانند با استفاده از مدارات کوانتومی تغییراتی آموزش ببینند. پارامترهای مدار به گونه‌ای بهینه می‌شوند که کلماتی که در زمینه‌های مشابه ظاهر می‌شوند حالت‌های کوانتومی مشابهی داشته باشند. شباهت میان کلمات می‌تواند با استفاده از معیارهای کوانتومی مانند فیدلیتی یا وفاداری
\ft{fidelity} 
اندازه‌گیری شود. این رویکرد می‌تواند از فضای نمایشی کوانتومی با ابعاد بالا بهره ببرد و احتمالاً روابط معنایی پیچیده‌تری را نسبت به روش‌های کلاسیک مدل کند.

\section{رویکرد DisCoCat: معناشناسی ترکیبی-توزیعی-مقوله‌ای}
\label{sec:discocat}

\subsection{مبانی نظری و فلسفه رویکرد}
\label{subsec:discocat-theory}

رویکرد ترکیبی-توزیعی-مقوله‌ای 
\ft{DisCoCat (Distributional Compositional Categorical)} 
که توسط کوک، سادرزاده و کلارک در سال 2010 معرفی شد \cite{coecke2010mathematical}، یکی از تأثیرگذارترین و پایه‌ای‌ترین چارچوب‌های ریاضی برای پردازش زبان طبیعی کوانتومی است. این رویکرد تلاش می‌کند تا پلی میان سه حوزه مهم ایجاد کند: معناشناسی توزیعی، دستور ترکیبی، و نظریه مقوله‌ها.

فلسفه اصلی ترکیبی-توزیعی-مقوله‌ای این است که معنای یک جمله از دو جنبه تشکیل می‌شود: اول، معنای کلمات فردی که از طریق رویکردهای توزیعی بر اساس زمینه استفاده آن‌ها به دست می‌آید. دوم، ساختار نحوی که نحوه ترکیب این معانی را مشخص می‌کند. نظریه مقوله‌ها زبان ریاضی مناسبی برای فرمال کردن این ترکیب فراهم می‌آورد.

در معناشناسی توزیعی
\ft{distributional semantics}، 
معنای یک کلمه بر اساس زمینه‌هایی که در آن ظاهر می‌شود تعریف می‌شود. این ایده بر اساس فرضیه توزیعی است که قبلاً ذکر شد: کلماتی که در زمینه‌های مشابه ظاهر می‌شوند معانی مشابهی دارند. در مدل‌های توزیعی، معنای هر کلمه با یک بردار در فضای برداری نمایش داده می‌شود.

ساختار ترکیبی 
\ft{compositional structure} 
بر این اصل استوار است که معنای یک عبارت پیچیده، تابعی از معنای اجزای آن و نحوه ترکیب آن‌هاست. این اصل که به اصل ترکیب‌پذیری فرگه معروف است، یک مفروضه بنیادی در زبان‌شناسی است.

نظریه مقوله‌ها 
\ft{category theory} 
یک شاخه از ریاضیات است که با ساختارهای انتزاعی و تبدیلات میان آن‌ها سروکار دارد. در این نظریه، یک مقوله شامل اشیاء و مورفیسم‌ها (نگاشت‌های) میان آن‌هاست. برای ترکیبی-توزیعی-مقوله‌ای، از نوع خاصی از مقوله‌ها به نام مقوله‌های فشرده متقارن یا مقوله‌های تک‌ریختی متقارن
\ft{symmetric monoidal categories} 
استفاده می‌شود که دارای عملگر ضرب تانسوری $\otimes$ هستند.

\subsection{دستور پیش‌گروهی}
\label{subsec:pregroup}

در پایه رویکرد ترکیبی-توزیعی-مقوله‌ای، دستور پیش‌گروهی یا دستور پیش‌گروهی
\lr{Pregroup Grammar} \cite{lambek2008type,preller2007free} 
قرار دارد که توسط لامبک  توسعه یافته است. در این دستور، هر کلمه یک تایپ نحوی دارد که نحوه ترکیب آن با کلمات دیگر را مشخص می‌کند.

تایپ‌های پایه یا اتمی عبارتند از $n$ برای اسم یا اسم
\ft{noun} 
و $s$ برای جمله یا جمله
\ft{sentence}. 
از این تایپ‌های پایه می‌توان تایپ‌های پیچیده‌تر ساخت. برای هر تایپ $A$، دو تایپ الحاقی یا الحاقی
\ft{adjoint} 
وجود دارد: $A^r$ که الحاقی راست
\ft{right adjoint} 
نامیده می‌شود و $A^l$ که الحاقی چپ
\ft{left adjoint} 
نامیده می‌شود. این تایپ‌های الحاقی نمایانگر این هستند که کلمه به چه نوع تایپی در سمت راست یا چپ خود نیاز دارد.

قوانین کاهش یا قوانین کاهش
\ft{reduction rules} 
در دستور پیش‌گروهی بسیار ساده هستند:
\begin{equation}
n \otimes n^r \rightarrow 1 \quad \text{و} \quad n^l \otimes n \rightarrow 1
\end{equation}
در این رابطه، $1$ نشان‌دهنده تایپ واحد یا خنثی است. این قوانین بیان می‌کنند که یک تایپ می‌تواند با تایپ الحاقی خود «حذف» یا «کاهش» شود.

به عنوان مثال، در زبان انگلیسی که ساختار فاعل-فعل-مفعول (\lr{SVO})
\ft{Subject-Verb-Object} 
دارد، یک فعل متعدی تایپ زیر را دارد:
\begin{equation}
n^r \otimes s \otimes n^l
\end{equation}
این تایپ بیان می‌کند که فعل برای تشکیل جمله نیاز به یک اسم در سمت چپ (فاعل) و یک اسم در سمت راست (مفعول) دارد. وقتی این فعل با فاعل و مفعولش ترکیب می‌شود، داریم:
\begin{equation}
n \otimes (n^r \otimes s \otimes n^l) \otimes n
\end{equation}
با اعمال قوانین کاهش:
\begin{equation}
n \otimes (n^r \otimes s \otimes n^l) \otimes n \rightarrow (n \otimes n^r) \otimes s \otimes (n^l \otimes n) \rightarrow 1 \otimes s \otimes 1 \rightarrow s
\end{equation}
که در نهایت به تایپ $s$ (جمله) می‌رسد.

\subsection{نمایش دیاگرامی و دیاگرام‌های رشته‌ای}
\label{subsec:string-diagrams}

یکی از ویژگی‌های قدرتمند و جذاب رویکرد ترکیبی-توزیعی-مقوله‌ای، استفاده از دیاگرام‌های رشته‌ای \ft{string diagrams} است. این دیاگرام‌ها که در کتاب مرجع "تصویرسازی فرآیندهای کوانتومی" \ft{Picturing Quantum Processes} \cite{coecke2017picturing} به تفصیل توضیح داده شده‌اند، نمایشی بصری و شهودی از ساختار نحوی و معنایی جملات فراهم می‌کنند.

در دیاگرام‌های رشته‌ای، کلمات به صورت جعبه‌هایی نمایش داده می‌شوند که از آن‌ها سیم‌هایی (رشته‌ها) بیرون می‌آیند. این سیم‌ها نمایانگر تایپ‌های نحوی هستند. جهت عمودی دیاگرام نمایانگر ترکیب توالی \ft{sequential composition} است که با علامت $\circ$ یا $>>$ نشان داده می‌شود. جهت افقی نمایانگر ضرب تانسوری \ft{tensor product} است که با $\otimes$ یا $@$ نمایش داده می‌شود.

عناصر اصلی در این دیاگرام‌ها عبارتند از: سیم \ft{Wire} که انتقال یک تایپ را نشان می‌دهد، فنجان \ft{Cup} که کاهش یا حذف دو تایپ الحاقی مجاور را نمایش می‌دهد، کلاهک \ft{Cap} که ایجاد یک جفت تایپ الحاقی را نشان می‌دهد، و تعویض \ft{Swap} که تعویض ترتیب دو سیم را نمایش می‌دهد.

برای مثال، جمله ساده انگلیسی "Alice loves Bob" را در نظر بگیرید. در این جمله، "Alice" و "Bob" هر کدام تایپ $n$ (اسم) دارند، و "loves" تایپ $n^r \otimes s \otimes n^l$ دارد. دیاگرام این جمله نشان می‌دهد که چگونه تایپ‌های اسم‌ها با تایپ‌های الحاقی فعل کاهش می‌یابند تا در نهایت تایپ $s$ (جمله) را تولید کنند. این نمایش بصری به فهم ساختار نحوی کمک می‌کند و همچنین راه را برای تبدیل به مدارات کوانتومی هموار می‌سازد.

مزیت اصلی دیاگرام‌های رشته‌ای این است که ارتباط مستقیمی با مدارات کوانتومی دارند. در واقع، دیاگرام‌های رشته‌ای و مدارات کوانتومی هر دو نمایش‌های بصری از مقوله‌های تک‌ریختی متقارن هستند، که این امر تبدیل میان آن‌ها را طبیعی و مستقیم می‌کند.

\subsection{نگاشت به فضای کوانتومی و تبدیل به مدار}
\label{subsec:quantum-mapping}

یکی از شگفت‌انگیزترین جنبه‌های رویکرد ترکیبی-توزیعی-مقوله‌ای این است که به طور طبیعی و مستقیم با محاسبات کوانتومی سازگار است. این سازگاری ناشی از این واقعیت است که هم دستور پیش‌گروهی و هم محاسبات کوانتومی بر پایه ریاضیات مقوله‌های تک‌ریختی متقارن استوارند.

در این نگاشت، تایپ‌های نحوی به فضاهای هیلبرت نگاشت می‌شوند. به عنوان مثال، تایپ پایه $n$ (اسم) به یک فضای هیلبرت مانند $\mathbb{C}^2$ (فضای دو کیوبیتی) نگاشت می‌شود، و تایپ $s$ (جمله) نیز به یک فضای هیلبرت مشابه. ضرب تانسوری نحوی $\otimes$ مستقیماً به ضرب تانسوری فضاهای هیلبرت نگاشت می‌شود. بنابراین اگر تایپ‌های $A$ و $B$ به فضاهای $H_A$ و $H_B$ نگاشت شوند، تایپ $A \otimes B$ به فضای $H_A \otimes H_B$ نگاشت می‌یابد.

کلمات که در ترکیبی-توزیعی-مقوله‌ای به عنوان بردارهای معنایی نمایش داده می‌شوند، در نسخه کوانتومی به حالت‌های کوانتومی یا عملگرهای کوانتومی تبدیل می‌شوند. برای مثال، یک اسم که در مدل توزیعی کلاسیک یک بردار در $\mathbb{R}^d$ است، می‌تواند به یک حالت کوانتومی در فضای هیلبرت مربوطه تبدیل شود. یک فعل که تایپ پیچیده‌تری دارد، به یک عملگر کوانتومی یا یک مدار کوانتومی پارامتری نگاشت می‌شود.

عناصر ساختاری دیاگرام‌های رشته‌ای نیز معادل‌های کوانتومی دارند. عملیات فنجان که دو تایپ الحاقی را کاهش می‌دهد، در فضای کوانتومی به حالت‌های درهم‌تنیده مانند حالت بل \ft{Bell state} و عملیات اندازه‌گیری نگاشت می‌شود. عملیات تعویض که ترتیب دو سیم را تعویض می‌کند، به دروازه تعویض کوانتومی \ft{SWAP gate} نگاشت می‌یابد. سیم‌ها که تایپ‌ها را منتقل می‌کنند، به عملگر همانی \ft{Identity} در فضای کوانتومی نگاشت می‌شوند.

این نگاشت طبیعی امکان تبدیل مستقیم یک دیاگرام نحوی ترکیبی-توزیعی-مقوله‌ای به یک مدار کوانتومی را فراهم می‌آورد. این فرآیند یک از مراحل کلیدی در پردازش زبان طبیعی کوانتومی است، زیرا پل میان تحلیل نحوی زبانی و اجرای کوانتومی را ایجاد می‌کند.

\subsection{کتابخانه لامبک : ابزار کامل برای پردازش زبان طبیعی کوانتومی}
\label{subsec:lambeq }
کتابخانه \lr{Lambeq}
 \cite{kartsaklis2021lambeq} یک کتابخانه پایتون منبع‌باز و جامع است که توسط تیم
کوانتینیوم
 \ft{Quantinuum}
  توسعه یافته و پیاده‌سازی کامل رویکرد ترکیبی-توزیعی-مقوله‌ای برای پردازش زبان طبیعی کوانتومی را فراهم می‌کند. این کتابخانه طراحی شده تا تمام مراحل پایپلاین \ft{pipeline} از جمله تبدیل متن به مدار کوانتومی، آموزش مدل، و ارزیابی را پوشش دهد.

معماری لامبک شامل چندین مؤلفه اصلی است که به صورت ماژولار طراحی شده‌اند. اولین مؤلفه، تحلیل‌گر \ft{Parser} است که وظیفه تبدیل جملات متنی به دیاگرام‌های نحوی را بر عهده دارد. برای زبان انگلیسی، \lr{BobcatParser} ارائه شده که بر اساس گرامر ترکیبی مقوله‌ای \lr{(CCG)}
 \ft{Combinatory Categorial Grammar}
 	 عمل می‌کند. این تحلیل‌گر جملات را تجزیه کرده و به دیاگرام‌های ترکیبی-توزیعی-مقوله‌ای تبدیل می‌کند. علاوه بر این، تحلیل‌گرهای دیگری نیز مانند \lr{SpacyTokeniser} و \lr{BagsOfWordsParser} برای کاربردهای خاص وجود دارند.

مؤلفه دوم، بازنویس \ft{Rewriter} است که وظیفه ساده‌سازی و بهینه‌سازی دیاگرام‌ها را دارد. دیاگرام‌های نحوی اولیه ممکن است پیچیده باشند و شامل عناصری مانند فنجان‌های متعدد یا ساختارهای اضافی. بازنویس‌ها این دیاگرام‌ها را ساده می‌کنند تا برای تبدیل به مدار کوانتومی مناسب‌تر باشند. مهم‌ترین بازنویس، \lr{RemoveCupsRewriter} است که عملیات‌های فنجان را حذف می‌کند و دیاگرام را به شکل خطی‌تری در می‌آورد.

سومین مؤلفه کلیدی، آنساتز است که دیاگرام‌های ساده‌شده را به مدارات کوانتومی پارامتری تبدیل می‌کند. لامبک چندین آنساتز مختلف ارائه می‌دهد، از جمله \lr{IQPAnsatz} \ft{Instantaneous Quantum Polynomial}، \lr{Sim14Ansatz}، و \lr{MPSAnsatz} \ft{Matrix Product State}. هر آنساتز استراتژی متفاوتی برای نگاشت دیاگرام به مدار دارد و برای کاربردهای خاص بهینه شده است. \lr{IQPAnsatz} به ویژه در کاربردهای پردازش زبان طبیعی کوانتومی محبوب است زیرا توازن خوبی میان قدرت نمایشی و پیچیدگی مدار دارد.

مؤلفه آموزش‌دهنده \ft{Trainer}، وظیفه آموزش پارامترهای مدار کوانتومی را بر عهده دارد. لامبک از الگوریتم‌های بهینه‌سازی مختلفی پشتیبانی می‌کند، از جمله \lr{QuantumTrainer} که برای آموزش بر روی سخت‌افزار کوانتومی طراحی شده، و \lr{PytorchTrainer} و \lr{JAXTrainer} که برای شبیه‌سازی کلاسیک کارآمد استفاده می‌شوند. این آموزش‌دهنده‌ها از توابع هزینه مختلفی مانند \lr{Binary Cross-Entropy} و \lr{Mean Squared Error} پشتیبانی می‌کنند.

در نهایت، پسانه \ft{Backend}، محیطی را برای اجرای مدارات کوانتومی فراهم می‌کند. لامبک با چندین پسانه سازگار است، از جمله شبیه‌سازهایی مانند \lr{NumpyBackend} برای آزمایش سریع، و پسانه‌های کوانتومی واقعی مانند \lr{pytket} که به پلتفرم‌های مختلف کوانتومی از جمله \lr{IBM Quantum}، \lr{Quantinuum}، و \lr{IonQ} متصل می‌شود. این انعطاف‌پذیری امکان آزمایش و توسعه بر روی شبیه‌ساز و سپس اجرا بر روی سخت‌افزار واقعی را فراهم می‌آورد.

\subsection{تحقیقات مرتبط و کاربردها}
\label{subsec:related-research}

از زمان معرفی رویکرد \lr{DisCoCat}، تحقیقات گسترده‌ای در زمینه کاربردهای آن انجام شده است. یکی از اولین و موفق‌ترین کاربردها، طبقه‌بندی متن و دسته‌بندی جملات بوده است. لورنز و همکاران در مقاله تأثیرگذار خود با عنوان "\lr{QNLP in Practice}" \cite{lorenz2023qnlp}  نشان دادند که مدل‌های مبتنی بر \lr{DisCoCat} می‌توانند در وظایف دسته‌بندی جملات کوتاه نتایج رقابتی با روش‌های کلاسیک داشته باشند، حتی با مجموعه داده‌های آموزشی کوچک‌تر. آن‌ها مدل خود را بر روی سخت‌افزار کوانتومی واقعی اجرا کردند و نشان دادند که رویکرد کوانتومی در عمل قابل پیاده‌سازی است.

تحلیل احساسات 
\ft{sentiment analysis}
نیز یکی دیگر از کاربردهای مورد بررسی بوده است. در این وظیفه، هدف تعیین احساس مثبت یا منفی یک جمله است. رویکرد \lr{DisCoCat} می‌تواند ساختار نحوی جمله را در نظر بگیرد و تأثیر کلمات منفی‌ساز یا تشدیدکننده را به طور صحیح مدل کند. برای مثال، تفاوت میان "فیلم خوب نبود" و "فیلم بد نبود" که از نظر لغوی مشابه هستند اما معانی متضادی دارند، می‌تواند با تحلیل صحیح ساختار نحوی تشخیص داده شود.

پاسخ به پرسش
\ft{question answering}
و استنتاج متنی
\ft{textual entailment}
 نیز در چند مطالعه بررسی شده‌اند. در این وظایف، مدل باید رابطه منطقی میان دو جمله را تعیین کند. دیاگرام‌های \lr{DisCoCat} می‌توانند ساختار معنایی هر دو جمله را نمایش دهند و مقایسه آن‌ها را تسهیل کنند.

مزایای اصلی رویکرد \lr{DisCoCat} نسبت به رویکردهای دیگر در چندین جنبه نمایان است. اول، قابلیت تفسیر بالا: دیاگرام‌های رشته‌ای قابل مشاهده و درک هستند و می‌توان به طور بصری ساختار نحوی و جریان معنا را دنبال کرد. این ویژگی در مقایسه با مدل‌های جعبه سیاه مانند شبکه‌های عصبی عمیق بسیار ارزشمند است. دوم، پایه ریاضی محکم: استفاده از نظریه مقوله‌ها چارچوب دقیق و رسمی برای استدلال در مورد معنا فراهم می‌کند. سوم، مدل‌سازی صریح ترکیب‌پذیری: برخلاف روش‌هایی که ساختار نحوی را به صورت ضمنی یاد می‌گیرند، \lr{DisCoCat} به طور مستقیم از دستور زبان استفاده می‌کند. چهارم، کارایی داده 
\ft{data efficiency}
 : به دلیل استفاده از ساختار نحوی، این روش معمولاً به داده‌های آموزشی کمتری نسبت به مدل‌های یادگیری عمیق نیاز دارد.

\section{پردازش زبان فارسی و QNLP}
\label{sec:persian-nlp}

\subsection{اهمیت و انگیزه تحقیق}
\label{subsec:persian-importance}

زبان فارسی یکی از زبان‌های کهن و غنی جهان است که توسط بیش از 110 میلیون نفر در ایران، افغانستان، تاجیکستان و جوامع ایرانی‌تبار سرتاسر جهان صحبت می‌شود. علی‌رغم اهمیت این زبان، منابع و ابزارهای پردازش زبان طبیعی فارسی به ویژه در حوزه‌های نوین مانند \lr{QNLP} به طور قابل توجهی محدودتر از زبان انگلیسی است. این شکاف فناوری، انگیزه قوی برای توسعه سیستم‌های \lr{QNLP} برای زبان فارسی فراهم می‌کند.

توسعه ابزارهای پردازش زبان فارسی کاربردهای عملی فراوانی دارد که به زندگی روزمره میلیون‌ها کاربر فارسی‌زبان مرتبط است. موتورهای جستجوی فارسی می‌توانند از درک بهتر پرسش‌های کاربران و محتوای صفحات وب فارسی بهره ببرند. ترجمه ماشینی فارسی-انگلیسی و بالعکس، که نقش مهمی در ارتباطات بین‌المللی و دسترسی به دانش جهانی دارد، می‌تواند از روش‌های پیشرفته‌تر سود ببرد. تحلیل احساسات در شبکه‌های اجتماعی فارسی برای درک افکار عمومی و بازخورد مشتریان اهمیت دارد. خلاصه‌سازی خودکار اخبار و اسناد فارسی می‌تواند به مدیریت بهتر سیل اطلاعات کمک کند. دستیارهای صوتی و چت‌بات‌های فارسی‌زبان می‌توانند خدمات بهتری به کاربران ارائه دهند. سیستم‌های پرسش و پاسخ فارسی می‌توانند دسترسی به اطلاعات را تسهیل کنند.

از منظر تحقیقاتی، توسعه \lr{QNLP} برای زبان فارسی چالش‌های منحصر به فردی را ارائه می‌دهد که حل آن‌ها می‌تواند به پیشرفت عمومی این حوزه کمک کند. تفاوت‌های ساختاری میان فارسی و انگلیسی، به ویژه در ترتیب کلمات و وجود عناصر نحوی خاص، نیازمند تطبیق و گاه بازتعریف رویکردهای موجود است. موفقیت در این زمینه نشان می‌دهد که رویکردهای \lr{QNLP} قابلیت تعمیم به زبان‌های متنوع را دارند و محدود به زبان انگلیسی نیستند.

\subsection{ویژگی‌های ساختاری و نحوی زبان فارسی}
\label{subsec:persian-features}

زبان فارسی دارای ویژگی‌های منحصر به فردی است که آن را از زبان انگلیسی متمایز می‌کند و چالش‌هایی برای پردازش خودکار ایجاد می‌نماید.

از نظر الفبا و نگارش، فارسی از الفبای عربی-فارسی استفاده می‌کند که در آن چهار حرف اضافی ("پ"، "چ"، "ژ"، "گ") نسبت به الفبای عربی وجود دارد. نوشتار فارسی از راست به چپ \lr{(RTL)} 
\ft{Right-to-Left}
است که برای نمایش و پردازش الگوریتمی نیازمند ابزارهای خاص است. حروف فارسی اشکال مختلفی بسته به موقعیت در کلمه (ابتدا، وسط، انتها یا منفرد) دارند که این موضوع توکنیزیشن در سطح کاراکتر را پیچیده می‌کند. علاوه بر این، مصوت‌های کوتاه (حروف کوتاه) معمولاً نوشته نمی‌شوند، که باعث ابهام در خواندن و پردازش می‌شود.

از نظر واژگانی، فارسی دارای سیستم غنی ساخت‌واژه است. کلمات مرکب در فارسی بسیار رایج هستند، مانند "نرم‌افزار"، "هوش مصنوعی"، "یادگیری ماشین" و "اشکال‌زدایی". این کلمات از دو یا چند جزء تشکیل شده‌اند که گاه با نیم‌فاصله و گاه بدون آن نوشته می‌شوند، که این امر مشکلاتی در توکنیزیشن ایجاد می‌کند. افعال مرکب نیز بسیار متداول هستند، که از یک جزء اسمی یا صفتی به همراه یک فعل کمکی (معمولاً "کردن"، "شدن" یا "داشتن") تشکیل می‌شوند، مانند "اجرا کردن"، "تصمیم گرفتن"، و "استفاده کردن". یک ویژگی مهم دیگر، اضافه کشیده است که با صدای "ِ" (کسره) نمایش داده می‌شود اما معمولاً نوشته نمی‌شود. این عنصر نحوی برای اتصال اسم به صفت، اسم به اسم، یا اسم به مکمل به کار می‌رود، مانند "کتاب جالب" (کتاب [اِ] جالب).

از نظر ساختار نحوی، مهم‌ترین تفاوت فارسی با انگلیسی، ترتیب اجزای جمله است. فارسی یک زبان  \lr{SOV} 
 \ft{Subject-Object-Verb}
  است، در حالی که انگلیسی
  \lr{SVO}
   است. بنابراین جمله‌ای مانند "مرد غذا را خورد" در انگلیسی به صورت 
  \lr{"The man ate the food"}
  می‌آید که ترتیب مفعول و فعل متفاوت است. این تفاوت تأثیر مستقیم بر تایپ‌گذاری دستور پیش‌گروهی دارد.

یکی از ویژگی‌های برجسته فارسی، وجود نشانگر مفعول "را" است که بعد از مفعول مستقیم معین می‌آید. این عنصر نحوی در انگلیسی وجود ندارد و تشخیص و مدل‌سازی آن در رویکرد \lr{DisCoCat} نیازمند تعریف تایپ نحوی جدید است. فارسی همچنین اجازه حذف ضمایر فاعلی را می‌دهد (زبان \lr{pro-drop})، بنابراین جملاتی مانند "رفتم" (من رفتم) بدون ذکر صریح فاعل کامل هستند. ترتیب صفت و موصوف نیز با انگلیسی متفاوت است: در فارسی صفت بعد از موصوف می‌آید ("کتاب بزرگ") در حالی که در انگلیسی قبل از آن است \lr{"big book"}.

\subsection{چالش‌های پردازش زبان فارسی و QNLP}
\label{subsec:persian-challenges}

پردازش خودکار زبان فارسی با چالش‌های متعددی مواجه است که برخی از آن‌ها عمومی و برخی دیگر خاص رویکرد کوانتومی هستند.

یکی از اساسی‌ترین چالش‌ها، ابهام در توکنیزیشن یا تقسیم متن به کلمات است. در فارسی، مرز میان کلمات همیشه با فاصله 
\ft{Space}
مشخص نمی‌شود. برخی کلمات مرکب با نیم‌فاصله نوشته می‌شوند، برخی بدون هیچ جداکننده‌ای، و برخی با فاصله کامل. این عدم یکنواختی باعث می‌شود تشخیص اینکه آیا دو کلمه مجاور باید به عنوان یک واحد یا دو واحد جداگانه در نظر گرفته شوند دشوار باشد. علاوه بر این، پیشوندها و پسوندها معمولاً به کلمه اصلی چسبیده‌اند و تشخیص آن‌ها نیازمند تحلیل ریخت‌شناسی است.

تحلیل ریخت‌شناسی فارسی پیچیدگی‌های خاص خود را دارد. فعل‌های فارسی بر اساس زمان (گذشته، حال، آینده)، شخص (اول، دوم، سوم) و شمار (مفرد، جمع) تصریف می‌شوند، که منجر به شکل‌های متعدد برای هر فعل می‌شود. برخی اسم‌ها جمع‌های غیرقیاسی دارند که باید به صورت واژگانی یاد گرفته شوند. سیستم ساخت‌واژه غنی فارسی امکان ایجاد کلمات جدید با ترکیب پیشوندها، پسوندها و عناصر مختلف را فراهم می‌کند که مدل‌سازی آن دشوار است.

کمبود منابع داده‌ای نیز یک مشکل جدی است. پیکره‌های زبانی فارسی که با برچسب‌های نحوی، نقش‌های معنایی یا سایر اطلاعات زبان‌شناختی حاشیه‌نویسی شده باشند، به طور قابل توجهی کوچک‌تر از معادل‌های انگلیسی هستند. این محدودیت بر توانایی آموزش مدل‌های داده‌محور تأثیر می‌گذارد. همچنین ابزارهای پردازش زبان فارسی نسبت به انگلیسی کمتر توسعه یافته و استاندارد شده هستند.

تنوع املایی در متون فارسی، به ویژه در محتوای تولید شده توسط کاربران در وب و شبکه‌های اجتماعی، چالش دیگری است. استانداردهای املایی همیشه رعایت نمی‌شوند، و تنوع در نوشتن برخی حروف مانند "ی" و "ی" یا "ک" و "ک" رایج است. مشکلات کدگذاری کاراکتر 
\ft{character encoding}
 نیز می‌تواند بروز کند، به ویژه تفاوت میان \lr{UTF-8} و فرم‌های نمایشی عربی.

برای \lr{QNLP} فارسی، چالش‌های خاصی وجود دارد. اولاً، عدم وجود یک پارسر
\ft{Parser}
  کوانتومی آماده برای فارسی. \lr{BobcatParser} که در لامبک ارائه شده تنها برای زبان انگلیسی است و نمی‌تواند مستقیماً برای فارسی استفاده شود. بنابراین نیاز به توسعه یک پارسر سفارشی برای فارسی وجود دارد، که کاری وقت‌گیر و نیازمند تخصص زبان‌شناختی است.

ثانیاً، تفاوت در ساختار نحوی نیاز به بازتعریف تایپ‌های دستور پیش‌گروهی دارد. در زبان انگلیسی \lr{SVO}، فعل متعدی تایپ $n^r \otimes s \otimes n^l$ دارد. اما در فارسی \lr{SOV}، این تایپ باید تطبیق یابد. علاوه بر این، وجود نشانگر مفعول "را" نیازمند تعریف یک تایپ نحوی جدید است که در دستور پیش‌گروهی استاندارد وجود ندارد. این تطبیقات نه تنها نیازمند درک عمیق دستور پیش‌گروهی، بلکه نیازمند درک دقیق ساختار نحوی فارسی نیز هستند.

ثالثاً، نمایش دوجهتی و نمایش دیاگرام‌ها برای متون \lr{RTL} چالش‌برانگیز است. ابزارهای موجود برای رسم دیاگرام‌های رشته‌ای معمولاً برای متون \lr{LTR} طراحی شده‌اند و نمایش صحیح فارسی نیازمند تنظیمات خاص یا توسعه ابزارهای جدید است. کتابخانه‌هایی مانند \lr{arabic-reshaper} و \lr{python-bidi} ممکن است برای این منظور لازم باشند.

\subsection{ابزارها و منابع موجود برای پردازش زبان فارسی}
\label{subsec:persian-resources}

علی‌رغم چالش‌های ذکر شده، چند ابزار و منبع مفید برای پردازش زبان فارسی در دسترس هستند که می‌توانند در توسعه سیستم‌های \lr{QNLP} فارسی مورد استفاده قرار گیرند.

مهم‌ترین کتابخانه پردازش زبان فارسی، هضم
 \ft{Hazm} 
 \cite{aleahmad2009hazm}
  است که یک کتابخانه پایتون جامع و منبع‌باز برای \lr{NLP} فارسی محسوب می‌شود. هضم مجموعه‌ای از ابزارهای مفید را ارائه می‌دهد که شامل نرمال‌ساز 
  \ft{Normalizer}
    برای یکنواخت‌سازی متن و حذف تنوع‌های املایی، توکنیزر کلمه 
    \ft{Word Tokenizer}
     برای جداسازی متن به کلمات، توکنیزر جمله 
     \ft{Sentence Tokenizer}
      برای تقسیم متن به جملات، برچسب‌زن اجزای کلام یا \lr{POS Tagger} که بر اساس مدل‌های آماری یا یادگیری ماشین کلمات را برچسب‌گذاری می‌کند، ریشه‌یاب 
      \ft{Lemmatizer}
       برای یافتن ریشه کلمات، و تحلیل‌گر وابستگی نحوی 
       \ft{Dependency Parser}
       برای استخراج ساختار نحوی جملات است. این ابزارها پایه و اساس بسیاری از سیستم‌های پردازش زبان فارسی هستند و در این تحقیق نیز مورد استفاده قرار گرفته‌اند.

علاوه بر هضم، ابزارهای دیگری نیز وجود دارند. \lr{ParsiNLU} یک مجموعه معیار (benchmark) و پایگاه داده برای وظایف مختلف \lr{NLP} فارسی ارائه می‌دهد که برای ارزیابی عملکرد مدل‌ها مفید است. ابزارهای مختلف دیگری نیز توسط محققان ایرانی در دانشگاه‌ها و مراکز تحقیقاتی توسعه یافته‌اند.

از نظر منابع داده، چند پیکره مهم در دسترس هستند. پیکره بیجنخان 
\ft{Bijankhan Corpus}
 یکی از اولین و معتبرترین پیکره‌های برچسب‌گذاری‌شده فارسی است که حاوی متون برچسب‌گذاری‌شده با اجزای کلام است. درخت وابستگی فارسی 
 \ft{Persian Dependency Treebank}
  \cite{seraji2016universal} که بر اساس چارچوب وابستگی جهانی
  \ft{Universal Dependencies}
   ساخته شده، حاوی جملات تجزیه‌شده نحوی است و برای آموزش پارسر ها بسیار ارزشمند است. همچنین دیتاست هایی برای وظایف خاص مانند شناسایی موجودیت‌های اسمی
  \lr{(NER)}
  \ft{Named Entity Recognition}
    در دسترس هستند.

\subsection{اهداف و چشم‌انداز این تحقیق}
\label{subsec:research-goals}

هدف اصلی این تحقیق، توسعه یک سیستم پردازش زبان طبیعی کوانتومی برای زبان فارسی بر اساس رویکرد \lr{DisCoCat} و با استفاده از کتابخانه لامبک است. این هدف کلی به اهداف فرعی مشخصی تقسیم می‌شود.

اول، طراحی و پیاده‌سازی یک پارسر سفارشی فارسی به نام \lr{PersianCatParser} است که مشابه \lr{BobcatParser} برای انگلیسی عمل کند. این parser باید بتواند جملات فارسی را به دیاگرام‌های \lr{DisCoCat} تبدیل کند. برای این منظور، لازم است تایپ‌های دستور پیش‌گروهی مناسب برای ساختار \lr{SOV} فارسی تعریف شوند. همچنین باید مکانیزمی برای مدیریت نشانگر مفعول "را" و سایر ویژگی‌های خاص فارسی مانند کلمات مرکب، افعال مرکب، و صفات پیاده‌سازی شود.

دوم، یکپارچه‌سازی با کتابخانه  هضم برای بهره‌برداری از ابزارهای موجود پردازش زبان فارسی است. از \lr{POS Tagger} هضم برای برچسب‌گذاری خودکار اجزای کلام استفاده می‌شود. نرمال‌سازی و پیش‌پردازش متن فارسی با استفاده از ابزارهای استاندارد هضم انجام می‌گیرد. همچنین از قابلیت‌های هضم برای شناسایی کلمات مرکب و افعال مرکب بهره‌برداری می‌شود.

سوم، آزمایش سیستم بر روی یک وظیفه واقعی \lr{NLP} برای ارزیابی عملکرد آن است. وظیفه انتخابی، دسته‌بندی جملات فارسی به دو دسته فناوری اطلاعات یا \lr{IT} و غذا یا \lr{Food} است. این وظیفه مشابه آزمایش‌های انجام شده برای انگلیسی است و امکان مقایسه را فراهم می‌کند. نتایج با روش‌های پایه 
\ft{baseline}
های کلاسیک مقایسه می‌شوند و تحلیل جامعی از نتایج، محدودیت‌ها و فرصت‌های بهبود ارائه می‌شود.

چهارم، این تحقیق شامل توسعه دو نسخه از سیستم است که به صورت تکاملی پیشرفت کرده‌اند. نسخه \lr{V00} یک پیاده‌سازی ساده با قوانین دستی و تایپ‌گذاری ثابت است که برای درک مفاهیم پایه و آزمایش ایده‌های اولیه طراحی شده است. نسخه \lr{V01} پیشرفته‌تر است و از کتابخانه  هضم برای پردازش زبانی استفاده می‌کند، قابلیت‌های بیشتری برای مدیریت ساختارهای پیچیده‌تر دارد، و کیفیت بهتری در تشخیص نقش‌های نحوی و پارس ارائه می‌دهد.

مشارکت‌های اصلی این تحقیق عبارتند از: ارائه  پارسر کوانتومی برای زبان فارسی بر اساس رویکرد \lr{DisCoCat}، تطبیق دستور پیش‌گروهی با ساختار نحوی \lr{SOV} فارسی و ویژگی‌های خاص این زبان، ایجاد یک دیتاست برچسب‌گذاری‌شده فارسی برای وظیفه دسته‌بندی که می‌تواند برای تحقیقات آینده مفید باشد، نشان دادن امکان‌پذیری و قابلیت تعمیم رویکرد \lr{DisCoCat} به زبان‌های غیرانگلیسی با ساختارهای نحوی متفاوت، و ارائه کدهای منبع‌باز و مستندسازی شده که می‌تواند مبنایی برای تحقیقات بعدی در این حوزه باشد.

\section{جمع‌بندی}
\label{sec:chapter1-conclusion}

در این فصل، سیری جامع از تاریخچه پردازش زبان طبیعی تا مرزهای جدید پردازش زبان طبیعی کوانتومی داشتیم. ابتدا تحولات پردازش زبان طبیعی را از دهه‌های میانی قرن بیستم تا امروز بررسی کردیم، از روش‌های مبتنی بر قاعده گرفته تا مدل‌های نمایش توزیعی کلمات، شبکه‌های عصبی بازگشتی، معماری ترنسفورمر، و در نهایت مدل‌های زبانی بزرگی که امروزه توانایی‌های شگفت‌انگیزی در فهم و تولید زبان از خود نشان می‌دهند.

سپس به رویکرد کوانتومی در پردازش زبان طبیعی پرداختیم. مبانی محاسبات کوانتومی شامل برهم‌نهی، درهم‌تنیدگی و تداخل را توضیح دادیم و اشاره کردیم که چگونه این ویژگی‌های منحصر به فرد می‌توانند برای مدل‌سازی زبان به کار گرفته شوند. مزایای بالقوه رویکرد کوانتومی از جمله نمایش طبیعی ترکیب‌پذیری و توانایی مدل‌سازی همبستگی‌های غیرکلاسیک را بررسی کردیم، و همچنین چالش‌های جدی آن در عصر \lr{NISQ} فعلی را مورد بحث قرار دادیم.

رویکرد \lr{DisCoCat} به عنوان یکی از قدرتمندترین و پایه‌ای‌ترین چارچوب‌های ریاضی برای پردازش زبان طبیعی کوانتومی معرفی شد. این رویکرد با ترکیب معناشناسی توزیعی، دستور پیش‌گروهی، و نظریه مقوله‌ها، پلی میان زبان‌شناسی و فیزیک کوانتومی ایجاد می‌کند. دیاگرام‌های رشته‌ای نمایشی بصری و شهودی از ساختار نحوی و معنایی جملات فراهم می‌کنند و به طور طبیعی به مدارات کوانتومی نگاشت می‌شوند. کتابخانه لامبک با ارائه ابزارهای کامل از پارسر گرفته تا پسانه های کوانتومی، پیاده‌سازی عملی این رویکرد را ممکن ساخته است.

در نهایت، به چالش‌های خاص و فرصت‌های پردازش زبان فارسی پرداختیم. ویژگی‌های منحصر به فرد زبان فارسی از جمله ساختار \lr{SOV}، نشانگر مفعول "را"، کلمات و افعال مرکب، و اضافه کشیده را تشریح کردیم. چالش‌های فنی مانند توکنیزیشن، کمبود منابع، و عدم وجود پارسر کوانتومی آماده برای فارسی را مورد بحث قرار دادیم. اهداف این تحقیق برای ایجاد یک سیستم \lr{QNLP} فارسی و مشارکت‌های مورد انتظار آن را تشریح نمودیم.

در فصل‌های بعدی، به جزئیات فنی خواهیم پرداخت. فصل دوم ابزارها و مفاهیم فنی پردازش زبان را بررسی می‌کند. فصل سوم به تفصیل به رویکرد \lr{DisCoCat} و دیاگرام‌های نحوی برای جملات فارسی می‌پردازد. فصل چهارم پیاده‌سازی، آموزش مدل، نتایج آزمایش‌ها و چشم‌انداز آینده را ارائه خواهد کرد.

